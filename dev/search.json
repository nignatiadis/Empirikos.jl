[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Empirikos",
    "section": "",
    "text": "Installation\nEmpirical Bayes estimation and inference in Julia.\nConsider \\(n\\) independent samples \\(Z_i\\) drawn from the following hierarchical model \\[\n\\mu_i \\sim G, \\ \\ Z_i \\sim p_i(\\cdot \\mid \\mu_i).\n\\] Here \\(G\\) is the unknown prior (effect size distribution) and \\(p_i(\\cdot \\mid \\mu_i),i=1,\\dotsc,n\\) are known likelihood functions.\nThis package provides a unified framework for estimation and inference under the above setting, which is known as the empirical Bayes problem (Robbins 1956).\nThe package is available from the Julia registry. It may be installed on Julia version 1.10 as follows:\nFor some of its functionality, this package requires a convex programming solver. The requirement for such a solver is that it can solve second order conic programs (SOCP), that it returns the dual variables associated with the SOCP constraints and that it is supported by JuMP.jl. We recommend using the MOSEK solver through the MosekTools.jl package. MOSEK is a commercial solver, but provides free academic licenses. An open-source alternative is Hypatia.jl.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "Empirikos",
    "section": "",
    "text": "using Pkg\nPkg.add(\"Empirikos\")\n\n\n\n\n\n\n\nModularity\n\n\n\nThis package has been designed with the goal of modularity. Specialized code (using Julia’s multiple dispatch) can be easily added to more efficiently handle different combinations of estimation targets, statistical algorithms, classes of priors and likelihoods. Please open an issue if there is a combination thereof that you would like to use (and which does not work currently or is slow).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "index.html#related-packages",
    "href": "index.html#related-packages",
    "title": "Empirikos",
    "section": "Related packages",
    "text": "Related packages\n\nIn R:\n\nREBayes (Koenker and Gu 2017)\nAshr (Stephens 2016)\nDeconvolveR (Narasimhan and Efron 2020)\nEbayesThresh (Johnstone and Silverman 2005)\n\n\n\nIn Julia:\n\nAurora.jl",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Empirikos",
    "section": "References",
    "text": "References\n\n\n\n\n\nJohnstone, Iain M, and Bernard W Silverman. 2005. “EbayesThresh: R and s-Plus Programs for Empirical Bayes Thresholding.” Journal of Statistical Software 12: 1–38.\n\n\nKoenker, Roger, and Jiaying Gu. 2017. “REBayes: Empirical Bayes Mixture Methods in r.” Journal of Statistical Software 82 (8): 1–26.\n\n\nNarasimhan, Balasubramanian, and Bradley Efron. 2020. “deconvolveR: A g-Modeling Program for Deconvolution and Empirical Bayes Estimation.” Journal of Statistical Software 94 (1): 1–20.\n\n\nRobbins, Herbert. 1956. “An Empirical Bayes Approach to Statistics.” Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics.\n\n\nStephens, Matthew. 2016. “False Discovery Rates: A New Deal.” Biostatistics 18 (2): 275–94.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "manual/samples.html",
    "href": "manual/samples.html",
    "title": "Empirical Bayes samples",
    "section": "",
    "text": "EBayesSample\nThe design choice of this package, is that each sample is wrapped in a type that represents its likelihood. This works well, since in the empirical Bayes problem, we typically impose (simple) assumptions on the distribution of \\(Z_i \\mid \\mu_i\\) and complexity emerges from making compound or nonparametric assumptions on the \\(\\mu_i\\) and sharing information across \\(i\\). The main advantage is that it then makes it easy to add new likelihoods and have it automatically integrate with the rest of the package (say the nonparametric maximum likelihood estimator) through Julia’s multiple dispatch.\nThe abstract type is",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Empirical Bayes samples</span>"
    ]
  },
  {
    "objectID": "manual/samples.html#ebayessample",
    "href": "manual/samples.html#ebayessample",
    "title": "Empirical Bayes samples",
    "section": "",
    "text": "EBayesSample{T}\nAbstract type representing empirical Bayes samples with realizations of type T.",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Empirical Bayes samples</span>"
    ]
  },
  {
    "objectID": "manual/samples.html#example-standardnormalsample",
    "href": "manual/samples.html#example-standardnormalsample",
    "title": "Empirical Bayes samples",
    "section": "Example: StandardNormalSample",
    "text": "Example: StandardNormalSample\nWe explain the interface in the most well-studied empirical Bayes setting, namely the Gaussian compound decision problem wherein \\(Z_i \\mid \\mu_i \\sim \\mathcal{N}(\\mu_i,1)\\). Such a sample is represented through the StandardNormalSample type:",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Empirical Bayes samples</span>"
    ]
  },
  {
    "objectID": "manual/samples.html#standardnormalsample",
    "href": "manual/samples.html#standardnormalsample",
    "title": "Empirical Bayes samples",
    "section": "StandardNormalSample",
    "text": "StandardNormalSample\n\n\n\n\n\n\nStandardNormalSample(Z)\nAn observed sample \\(Z\\) drawn from a Normal distribution with known variance \\(\\sigma^2 =1\\).\n\\[\nZ \\sim \\mathcal{N}(\\mu, 1)\n\\]\n\\(\\mu\\) is assumed unknown. The type above is used when the sample \\(Z\\) is to be used for estimation or inference of \\(\\mu\\).\njulia&gt; StandardNormalSample(0.5)          #Z=0.5\nN(0.5; μ, σ=1.0)\n\n\n\nThe type can be used in three ways. First, say we observe \\(Z_i=1.0\\), then we reprent that as Z = StandardNormalSample(1.0). A more advanced functionality consists of StandardNormalSample(missing), which represents the random variable \\(Z_i\\) without having observed its realization yet.",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Empirical Bayes samples</span>"
    ]
  },
  {
    "objectID": "manual/samples.html#interface",
    "href": "manual/samples.html#interface",
    "title": "Empirical Bayes samples",
    "section": "Interface",
    "text": "Interface\nThe main interface functions are the following:",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Empirical Bayes samples</span>"
    ]
  },
  {
    "objectID": "manual/samples.html#likelihood_distribution",
    "href": "manual/samples.html#likelihood_distribution",
    "title": "Empirical Bayes samples",
    "section": "likelihood_distribution",
    "text": "likelihood_distribution\n\n\n\n\n\n\nlikelihood_distribution(Z::EBayesSample, μ::Number)\nReturns the distribution \\(p(\\cdot \\mid \\mu)\\) of \\(Z \\mid \\mu\\) (the return type being a Distributions.jl Distribution).\n\nExamples\njulia&gt; likelihood_distribution(StandardNormalSample(1.0), 2.0)\nNormal{Float64}(μ=2.0, σ=1.0)",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Empirical Bayes samples</span>"
    ]
  },
  {
    "objectID": "manual/samples.html#response",
    "href": "manual/samples.html#response",
    "title": "Empirical Bayes samples",
    "section": "response",
    "text": "response\n\n\n\n\n\n\nresponse(model::RegressionModel)\nReturn the model response (a.k.a. the dependent variable).\nresponse(Z::EBayesSample{T})\nReturns the concrete realization of Z as type T, thus dropping the information about the likelihood.\n\nExamples\njulia&gt; response(StandardNormalSample(1.0))\n1.0",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Empirical Bayes samples</span>"
    ]
  },
  {
    "objectID": "manual/samples.html#marginalize",
    "href": "manual/samples.html#marginalize",
    "title": "Empirical Bayes samples",
    "section": "marginalize",
    "text": "marginalize\n\n\n\n\n\n\nmarginalize(Z::EBayesSample, prior::Distribution)\nGiven a prior distribution \\(G\\) and EBayesSample \\(Z\\), return that marginal distribution of \\(Z\\). Works for `EBayesSample{Missing}``, i.e., no realization is needed.\n\nExamples\n`jldoctest julia&gt; marginalize(StandardNormalSample(1.0), Normal(2.0, sqrt(3))) Normal{Float64}(μ=2.0, σ=1.9999999999999998)``",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Empirical Bayes samples</span>"
    ]
  },
  {
    "objectID": "manual/samples.html#pdf",
    "href": "manual/samples.html#pdf",
    "title": "Empirical Bayes samples",
    "section": "pdf",
    "text": "pdf\n\n\n\n\n\n\npdf(d::Distribution{ArrayLikeVariate{N}}, x::AbstractArray{&lt;:Real,N}) where {N}\nEvaluate the probability density function of d at x.\nThis function checks if the size of x is compatible with distribution d. This check can be disabled by using @inbounds.\n\nImplementation\nInstead of pdf one should implement _pdf(d, x) which does not have to check the size of x. However, since the default definition of pdf(d, x) falls back to logpdf(d, x) usually it is sufficient to implement logpdf.\nSee also: logpdf.\npdf(d::Distribution{ArrayLikeVariate{N}}, x) where {N}\nEvaluate the probability density function of d at every element in a collection x.\nThis function checks for every element of x if its size is compatible with distribution d. This check can be disabled by using @inbounds.\nHere, x can be\n\nan array of dimension &gt; N with size(x)[1:N] == size(d), or\nan array of arrays xi of dimension N with size(xi) == size(d).\n\npdf(d::UnivariateDistribution, x::Real)\nEvaluate the probability density (mass) at x.\nSee also: logpdf.\npdf(d::Union{UnivariateMixture, MultivariateMixture}, x)\nEvaluate the (mixed) probability density function over x. Here, x can be a single sample or an array of multiple samples.\npdf(prior::Distribution, Z::EBayesSample)\nGiven a prior \\(G\\) and EBayesSample \\(Z\\), compute the marginal density of Z.\n\n\nExamples\njulia&gt; Z = StandardNormalSample(1.0)\nN(1.0; μ, σ=1.0)\njulia&gt; prior = Normal(2.0, sqrt(3))\nNormal{Float64}(μ=2.0, σ=1.7320508075688772)\njulia&gt; pdf(prior, Z)\n0.17603266338214976\njulia&gt; pdf(Normal(2.0, 2.0), 1.0)\n0.17603266338214976",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Empirical Bayes samples</span>"
    ]
  },
  {
    "objectID": "manual/samples.html#cdf",
    "href": "manual/samples.html#cdf",
    "title": "Empirical Bayes samples",
    "section": "cdf",
    "text": "cdf\n\n\n\n\n\n\ncdf(d::UnivariateDistribution, x::Real)\nEvaluate the cumulative probability at x.\nSee also ccdf, logcdf, and logccdf.\ncdf(d::Skellam, t::Real)\nImplementation based on SciPy: https://github.com/scipy/scipy/blob/v0.15.1/scipy/stats/discretedistns.py\nRefer to Eqn (5) in On an Extension of the Connexion Between Poisson and χ2 Distributions, N.L Johnson(1959) Vol 46, No 3/4, doi:10.2307/2333532 It relates the Skellam and Non-central chisquare PDFs, which is very similar to their CDFs computation as well.\nComputing cdf of the Skellam distribution.\ncdf(prior::Distribution, Z::EBayesSample)\nGiven a prior \\(G\\) and EBayesSample \\(Z\\), evaluate the CDF of the marginal distribution of \\(Z\\) at response(Z).",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Empirical Bayes samples</span>"
    ]
  },
  {
    "objectID": "manual/samples.html#other-implemented-ebayessample-types",
    "href": "manual/samples.html#other-implemented-ebayessample-types",
    "title": "Empirical Bayes samples",
    "section": "Other implemented EBayesSample types",
    "text": "Other implemented EBayesSample types\nCurrently, the following samples have been implemented.",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Empirical Bayes samples</span>"
    ]
  },
  {
    "objectID": "manual/samples.html#normalsample",
    "href": "manual/samples.html#normalsample",
    "title": "Empirical Bayes samples",
    "section": "NormalSample",
    "text": "NormalSample\n\n\n\n\n\n\nNormalSample(Z,σ)\nAn observed sample \\(Z\\) drawn from a Normal distribution with known variance \\(\\sigma^2 &gt; 0\\).\n\\[\nZ \\sim \\mathcal{N}(\\mu, \\sigma^2)\n\\]\n\\(\\mu\\) is assumed unknown. The type above is used when the sample \\(Z\\) is to be used for estimation or inference of \\(\\mu\\).\njulia&gt; NormalSample(0.5, 1.0)          #Z=0.5, σ=1\nN(0.5; μ, σ=1.0)",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Empirical Bayes samples</span>"
    ]
  },
  {
    "objectID": "manual/samples.html#binomialsample",
    "href": "manual/samples.html#binomialsample",
    "title": "Empirical Bayes samples",
    "section": "BinomialSample",
    "text": "BinomialSample\n\n\n\n\n\n\nBinomialSample(Z, n)\nAn observed sample \\(Z\\) drawn from a Binomial distribution with n trials.\n\\[\nZ \\sim \\text{Binomial}(n, p)\n\\]\n\\(p\\) is assumed unknown. The type above is used when the sample \\(Z\\) is to be used for estimation or inference of \\(p\\).\njulia&gt; BinomialSample(2, 10)          # 2 out of 10 trials successful\nℬ𝒾𝓃(2; p, n=10)",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Empirical Bayes samples</span>"
    ]
  },
  {
    "objectID": "manual/samples.html#poissonsample",
    "href": "manual/samples.html#poissonsample",
    "title": "Empirical Bayes samples",
    "section": "PoissonSample",
    "text": "PoissonSample\n\n\n\n\n\n\nPoissonSample(Z, E)\nAn observed sample \\(Z\\) drawn from a Poisson distribution,\n\\[\nZ \\sim \\text{Poisson}(\\mu \\cdot E).\n\\]\nThe multiplying intensity \\(E\\) is assumed to be known (and equal to 1.0 by default), while \\(\\mu\\) is assumed unknown. The type above is used when the sample \\(Z\\) is to be used for estimation or inference of \\(\\mu\\).\njulia&gt; PoissonSample(3)\n𝒫ℴ𝒾(3; μ)\njulia&gt; PoissonSample(3, 1.5)\n𝒫ℴ𝒾(3; μ⋅1.5)",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Empirical Bayes samples</span>"
    ]
  },
  {
    "objectID": "manual/estimation.html",
    "href": "manual/estimation.html",
    "title": "Prior Estimation",
    "section": "",
    "text": "EBayesMethod",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Prior Estimation</span>"
    ]
  },
  {
    "objectID": "manual/estimation.html#ebayesmethod",
    "href": "manual/estimation.html#ebayesmethod",
    "title": "Prior Estimation",
    "section": "",
    "text": "Abstract type representing empirical Bayes estimation methods.",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Prior Estimation</span>"
    ]
  },
  {
    "objectID": "manual/estimation.html#nonparametric-estimation",
    "href": "manual/estimation.html#nonparametric-estimation",
    "title": "Prior Estimation",
    "section": "Nonparametric estimation",
    "text": "Nonparametric estimation\nThe typical call for estimating the prior \\(G\\) based on empirical Bayes samples Zs is the following,\nStatsBase.fit(method, Zs)\nAbove, method is a type that specifies both the assumptions made on \\(G\\) (say, the convex prior class \\(\\mathcal{G}\\) in which \\(G\\) lies), as well as details concerning the computation (typically a JuMP.jl compatible convex programming solver).\n\nNonparametric Maximum Likelihood estimation (NPMLE)\nFor example, let us consider the nonparametric maximum likelihood estimator:",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Prior Estimation</span>"
    ]
  },
  {
    "objectID": "manual/estimation.html#npmle",
    "href": "manual/estimation.html#npmle",
    "title": "Prior Estimation",
    "section": "NPMLE",
    "text": "NPMLE\n\n\n\n\n\n\nNPMLE(convexclass, solver) &lt;: Empirikos.EBayesMethod\nGiven \\(n\\) independent samples \\(Z_i\\) from the empirical Bayes problem with prior \\(G\\) known to lie in the convexclass \\(\\mathcal{G}\\), estimate \\(G\\) by Nonparametric Maximum Likelihood (NPMLE)\n\\[\n\\widehat{G}_n \\in \\operatorname{argmax}_{G \\in \\mathcal{G}}\\left\\{\\sum_{i=1}^n \\log( f_{i,G}(Z_i)) \\right\\},\n\\]\nwhere \\(f_{i,G}(z) = \\int p_i(z \\mid \\mu) dG(\\mu)\\) is the marginal density of the \\(i\\)-th sample. The optimization is conducted by a JuMP compatible solver.\n\n\n\nSuppose we have Poisson samples Zs, each with a different mean \\(\\mu_i\\) drawn from \\(G=U[1,5]\\):\nusing Distributions\nn = 1000\nμs = rand(Uniform(1,5), n)\nZs = PoissonSample.(rand.(Poisson.(μs)))\nWe can then estimate \\(G\\) as follows using Mosek:\nusing MosekTools\ng_hat = fit(NPMLE(DiscretePriorClass(), Mosek.Optimizer), Zs)\nOr we can use the open-source Hypatia.jl solver:\nusing Hypatia\ng_hat = fit(NPMLE(DiscretePriorClass(), Hypatia.Optimizer), Zs)\n\nOther available nonparametric methods",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Prior Estimation</span>"
    ]
  },
  {
    "objectID": "manual/estimation.html#kolmogorovsmirnovminimumdistance",
    "href": "manual/estimation.html#kolmogorovsmirnovminimumdistance",
    "title": "Prior Estimation",
    "section": "KolmogorovSmirnovMinimumDistance",
    "text": "KolmogorovSmirnovMinimumDistance\n\n\n\n\n\n\nKolmogorovSmirnovMinimumDistance(convexclass, solver) &lt;: Empirikos.EBayesMethod\nGiven \\(n\\) i.i.d. samples from the empirical Bayes problem with prior \\(G\\) known to lie in the convexclass \\(\\mathcal{G}\\) , estimate \\(G\\) as follows:\n\\[\n\\widehat{G}_n \\in \\operatorname{argmin}_{G \\in \\mathcal{G}}\\{\\sup_{t \\in \\mathbb R}\\lvert F_G(t) - \\widehat{F}_n(t)\\rvert\\},\n\\]\nwhere \\(\\widehat{F}_n\\) is the ECDF of the samples. The optimization is conducted by a JuMP compatible solver.",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Prior Estimation</span>"
    ]
  },
  {
    "objectID": "manual/intervals.html",
    "href": "manual/intervals.html",
    "title": "Confidence intervals",
    "section": "",
    "text": "F-Localization based intervals\nHere we describe two methods for forming confidence intervals for empirical Bayes estimands.",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "manual/intervals.html#flocalizationinterval",
    "href": "manual/intervals.html#flocalizationinterval",
    "title": "Confidence intervals",
    "section": "FLocalizationInterval",
    "text": "FLocalizationInterval\n\n\n\n\n\n\nFLocalizationInterval(flocalization::Empirikos.FLocalization,\n                      convexclass::Empirikos.ConvexPriorClass,\n                      solver,\n                      n_bisection = 100,\n                      optimization_method = nothing)\nMethod for computing frequentist confidence intervals for empirical Bayes estimands. Here flocalization is a Empirikos.FLocalization, convexclass is a Empirikos.ConvexPriorClass, solver is a JuMP.jl compatible solver.\nn_bisection is relevant only for combinations of target, flocalization and convexclass for which the Charnes-Cooper transformation is not applicable/implemented. Instead, a quasi-convex optimization problem is solved by bisection and increasing n_bisection increases accuracy (at the cost of more computation).\noptimization_method determines how the optimization problem is solved. If nothing, the default optimization method of the solver is used. If CharnesCooper, the Charnes-Cooper transformation is used. If QuasiConvexBisection, a quasi-convex optimization problem is solved by bisection.\n\nReferences\nIgnatiadis and Wager (2022)",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "manual/intervals.html#amari-intervals",
    "href": "manual/intervals.html#amari-intervals",
    "title": "Confidence intervals",
    "section": "AMARI intervals",
    "text": "AMARI intervals",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "manual/intervals.html#amari",
    "href": "manual/intervals.html#amari",
    "title": "Confidence intervals",
    "section": "AMARI",
    "text": "AMARI\n\n\n\n\n\n\nAMARI(convexclass::Empirikos.ConvexPriorClass,\n      flocalization::Empirikos.FLocalization,\n      solver,\n      plugin_G = KolmogorovSmirnovMinimumDistance(convexclass, solver))\nAffine Minimax Anderson-Rubin intervals for empirical Bayes estimands. Here flocalization is a pilot Empirikos.FLocalization, convexclass is a Empirikos.ConvexPriorClass, solver is a JuMP.jl compatible solver. plugin_G is a Empirikos.EBayesMethod used as an initial estimate of the marginal distribution of the i.i.d. samples \\(Z\\).\n\nReferences\nIgnatiadis and Wager (2022)",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "manual/intervals.html#interface",
    "href": "manual/intervals.html#interface",
    "title": "Confidence intervals",
    "section": "Interface",
    "text": "Interface",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "manual/intervals.html#confint",
    "href": "manual/intervals.html#confint",
    "title": "Confidence intervals",
    "section": "confint",
    "text": "confint\n\n\n\n\n\n\nconfint(model::StatisticalModel; level::Real=0.95)\nCompute confidence intervals for coefficients, with confidence level level (by default 95%).\nStatsBase.confint(method::AMARI,\n                  target::Empirikos.EBayesTarget,\n                  Zs;\n                  level=0.95)\nForm a confidence interval for the Empirikos.EBayesTarget target with coverage level based on the samples Zs using the AMARI method.\n\n\n\n\n\n\n\nIgnatiadis, Nikolaos, and Stefan Wager. 2022. “Confidence Intervals for Nonparametric Empirical Bayes Analysis (with Discussion and a Rejoinder by the Authors).” Journal of the American Statistical Association 117 (539): 1149–66. https://doi.org/10.1080/01621459.2021.2008403.",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "manual/estimands.html",
    "href": "manual/estimands.html",
    "title": "Estimands",
    "section": "",
    "text": "EBayesTarget\nThis package defines dedicated types to describe empirical Bayes estimands (that can be used for estimation or inference).",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimands</span>"
    ]
  },
  {
    "objectID": "manual/estimands.html#ebayestarget",
    "href": "manual/estimands.html#ebayestarget",
    "title": "Estimands",
    "section": "",
    "text": "Abstract type that describes Empirical Bayes estimands (which we want to estimate or conduct inference for).",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimands</span>"
    ]
  },
  {
    "objectID": "manual/estimands.html#example-posteriormean",
    "href": "manual/estimands.html#example-posteriormean",
    "title": "Estimands",
    "section": "Example: PosteriorMean",
    "text": "Example: PosteriorMean",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimands</span>"
    ]
  },
  {
    "objectID": "manual/estimands.html#posteriormean",
    "href": "manual/estimands.html#posteriormean",
    "title": "Estimands",
    "section": "PosteriorMean",
    "text": "PosteriorMean\n\n\n\n\n\n\nPosteriorMean(Z::EBayesSample) &lt;: AbstractPosteriorTarget\nType representing the posterior mean, i.e.,\n\\[\nE_G[\\mu_i \\mid Z_i = z]\n\\]\n\n\n\nA target::EBayesTarget, such as PosteriorMean, may be used as a callable on distributions (priors).\njulia&gt; G = Normal()\nNormal{Float64}(μ=0.0, σ=1.0)\n\njulia&gt; postmean1 = PosteriorMean(StandardNormalSample(1.0))\njulia&gt; postmean1(G)\n0.5\n\njulia&gt; postmean2 = PosteriorMean(NormalSample(1.0, sqrt(3.0)))\njulia&gt; postmean2(G)\n0.25000000000000006",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimands</span>"
    ]
  },
  {
    "objectID": "manual/estimands.html#posterior-estimands",
    "href": "manual/estimands.html#posterior-estimands",
    "title": "Estimands",
    "section": "Posterior estimands",
    "text": "Posterior estimands\nIn addition to PosteriorMean, other implemented posterior estimands are the following:",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimands</span>"
    ]
  },
  {
    "objectID": "manual/estimands.html#posteriorprobability",
    "href": "manual/estimands.html#posteriorprobability",
    "title": "Estimands",
    "section": "PosteriorProbability",
    "text": "PosteriorProbability\n\n\n\n\n\n\nPosteriorProbability(Z::EBayesSample, s) &lt;: AbstractPosteriorTarget\nType representing the posterior probability, i.e.,\n\\[\n\\Prob_G[\\mu_i \\in s \\mid Z_i = z]\n\\]",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimands</span>"
    ]
  },
  {
    "objectID": "manual/estimands.html#posteriorvariance",
    "href": "manual/estimands.html#posteriorvariance",
    "title": "Estimands",
    "section": "PosteriorVariance",
    "text": "PosteriorVariance\n\n\n\n\n\n\nPosteriorVariance(Z::EBayesSample) &lt;: AbstractPosteriorTarget\nType representing the posterior variance, i.e.,\n\\[\nV_G[\\mu_i \\mid Z_i = z]\n\\]",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimands</span>"
    ]
  },
  {
    "objectID": "manual/estimands.html#linear-functionals",
    "href": "manual/estimands.html#linear-functionals",
    "title": "Estimands",
    "section": "Linear functionals",
    "text": "Linear functionals\nA special case of empirical Bayes estimands are linear functionals:",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimands</span>"
    ]
  },
  {
    "objectID": "manual/estimands.html#linearebayestarget",
    "href": "manual/estimands.html#linearebayestarget",
    "title": "Estimands",
    "section": "LinearEBayesTarget",
    "text": "LinearEBayesTarget\n\n\n\n\n\n\nLinearEBayesTarget &lt;: EBayesTarget\nAbstract type that describes Empirical Bayes estimands that are linear functionals of the prior G.\n\n\n\nCurrently available linear functionals:",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimands</span>"
    ]
  },
  {
    "objectID": "manual/estimands.html#priordensity",
    "href": "manual/estimands.html#priordensity",
    "title": "Estimands",
    "section": "PriorDensity",
    "text": "PriorDensity\n\n\n\n\n\n\nPriorDensity(z::Float64) &lt;: LinearEBayesTarget\n\nExample call\njulia&gt; PriorDensity(2.0)\nPriorDensity{Float64}(2.0)\n\n\nDescription\nThis is the evaluation functional of the density of \\(G\\) at z, i.e., \\(L(G) = G'(z) = g(z)\\) or in Julia code L(G) = pdf(G, z).",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimands</span>"
    ]
  },
  {
    "objectID": "manual/estimands.html#marginaldensity",
    "href": "manual/estimands.html#marginaldensity",
    "title": "Estimands",
    "section": "MarginalDensity",
    "text": "MarginalDensity\n\n\n\n\n\n\nMarginalDensity(Z::EBayesSample) &lt;: LinearEBayesTarget\n\nExample call\nMarginalDensity(StandardNormalSample(2.0))\n\n\nDescription\nDescribes the marginal density evaluated at \\(Z=z\\) (e.g. \\(Z=2\\) in the example above). In the example above the sample is drawn from the hierarchical model\n\\[\n\\mu \\sim G, Z \\sim \\mathcal{N}(0,1)\n\\]\nIn other words, letting \\(\\varphi\\) the Standard Normal pdf\n\\[\nL(G) = \\varphi \\star dG(z)\n\\]\nNote that 2.0 has to be wrapped inside StandardNormalSample(2.0) since this target depends not only on G and the location, but also on the likelihood.\n\n\n\n\nPosterior estimands such as PosteriorMean can be typically decomposed into two linear functionals, a numerator and a denominator:",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimands</span>"
    ]
  },
  {
    "objectID": "manual/estimands.html#numerator",
    "href": "manual/estimands.html#numerator",
    "title": "Estimands",
    "section": "numerator",
    "text": "numerator\n\n\n\n\n\n\nnumerator(x)\nNumerator of the rational representation of x.\n\nExamples\njulia&gt; numerator(2//3)\n2\n\njulia&gt; numerator(4)\n4\nBase.numerator(target::AbstractPosteriorTarget)\nSuppose a posterior target \\(\\theta_G(z)\\), such as the posterior mean can be written as:\n\\[\n\\theta_G(z) = \\frac{ a_G(z)}{f_G(z)} = \\frac{ \\int h(\\mu)dG(\\mu)}{\\int p(z \\mid \\mu)dG(\\mu)}.\n\\]\nFor example, for the posterior mean \\(h(\\mu) =  \\mu \\cdot p(z \\mid \\mu)\\). Then Base.numerator returns the linear functional representing \\(G \\mapsto a_G(z)\\).",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimands</span>"
    ]
  },
  {
    "objectID": "manual/estimands.html#denominator",
    "href": "manual/estimands.html#denominator",
    "title": "Estimands",
    "section": "denominator",
    "text": "denominator\n\n\n\n\n\n\ndenominator(x)\nDenominator of the rational representation of x.\n\nExamples\njulia&gt; denominator(2//3)\n3\n\njulia&gt; denominator(4)\n1\nBase.denominator(target::AbstractPosteriorTarget)\nSuppose a posterior target \\(\\theta_G(z)\\), such as the posterior mean can be written as:\n\\[\n\\theta_G(z) = \\frac{ a_G(z)}{f_G(z)} = \\frac{ \\int h(\\mu)dG(\\mu)}{\\int p(z \\mid \\mu)dG(\\mu)}.\n\\]\nFor example, for the posterior mean \\(h(\\mu) =  \\mu \\cdot p(z \\mid \\mu)\\). Then Base.denominator returns the linear functional representing \\(G \\mapsto f_G(z)\\) (i.e., typically the marginal density). Also see Base.numerator(::AbstractPosteriorTarget).",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimands</span>"
    ]
  },
  {
    "objectID": "manual/convexpriors.html",
    "href": "manual/convexpriors.html",
    "title": "Convex prior classes",
    "section": "",
    "text": "ConvexPriorClass\nThe starting point for many empirical Bayes tasks, such as inference or estimation, is to posit that the true prior \\(G\\) lies in a convex class of priors \\(\\mathcal{G}\\). Such classes of priors are represented in this package through the abstract type,\nCurrently, the following choices for \\(\\mathcal{G}\\) are available:",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convex prior classes</span>"
    ]
  },
  {
    "objectID": "manual/convexpriors.html#convexpriorclass",
    "href": "manual/convexpriors.html#convexpriorclass",
    "title": "Convex prior classes",
    "section": "",
    "text": "Abstract type representing convex classes of probability distributions \\(\\mathcal{G}\\).",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convex prior classes</span>"
    ]
  },
  {
    "objectID": "manual/convexpriors.html#discretepriorclass",
    "href": "manual/convexpriors.html#discretepriorclass",
    "title": "Convex prior classes",
    "section": "DiscretePriorClass",
    "text": "DiscretePriorClass\n\n\n\n\n\n\nDiscretePriorClass(support) &lt;: Empirikos.ConvexPriorClass\nType representing the family of all discrete distributions supported on a subset of support, i.e., it represents all DiscreteNonParametric distributions with support = support and probs taking values on the probability simplex.\nNote that DiscretePriorClass(support)(probs) == DiscreteNonParametric(support, probs).\n\nExamples\njulia&gt; gcal = DiscretePriorClass([0,0.5,1.0])\nDiscretePriorClass | support = [0.0, 0.5, 1.0]\n\njulia&gt; gcal([0.2,0.2,0.6])\nDiscreteNonParametric{Float64, Float64, Vector{Float64}, Vector{Float64}}(support=[0.0, 0.5, 1.0], p=[0.2, 0.2, 0.6])",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convex prior classes</span>"
    ]
  },
  {
    "objectID": "manual/convexpriors.html#mixturepriorclass",
    "href": "manual/convexpriors.html#mixturepriorclass",
    "title": "Convex prior classes",
    "section": "MixturePriorClass",
    "text": "MixturePriorClass\n\n\n\n\n\n\nMixturePriorClass(components) &lt;: Empirikos.ConvexPriorClass\nType representing the family of all mixture distributions with mixing components equal to components, i.e., it represents all MixtureModel distributions with components = components and probs taking values on the probability simplex.\nNote that MixturePriorClass(components)(probs) == MixtureModel(components, probs).\n\nExamples\njulia&gt; gcal = MixturePriorClass([Normal(0,1), Normal(0,2)])\nMixturePriorClass (K = 2)\nNormal{Float64}(μ=0.0, σ=1.0)\nNormal{Float64}(μ=0.0, σ=2.0)\n\njulia&gt; gcal([0.2,0.8])\nMixtureModel{Normal{Float64}}(K = 2)\ncomponents[1] (prior = 0.2000): Normal{Float64}(μ=0.0, σ=1.0)\ncomponents[2] (prior = 0.8000): Normal{Float64}(μ=0.0, σ=2.0)",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convex prior classes</span>"
    ]
  },
  {
    "objectID": "manual/convexpriors.html#gaussianscalemixtureclass",
    "href": "manual/convexpriors.html#gaussianscalemixtureclass",
    "title": "Convex prior classes",
    "section": "GaussianScaleMixtureClass",
    "text": "GaussianScaleMixtureClass\n\n\n\n\n\n\nGaussianScaleMixtureClass(σs) &lt;: Empirikos.ConvexPriorClass\nType representing the family of mixtures of Gaussians with mean 0 and standard deviations equal to σs. GaussianScaleMixtureClass(σs) represents the same class of distributions as MixturePriorClass.(Normal.(0, σs))\njulia&gt; gcal = GaussianScaleMixtureClass([1.0,2.0])\nGaussianScaleMixtureClass | σs = [1.0, 2.0]\n\njulia&gt; gcal([0.2,0.8])\nMixtureModel{Normal{Float64}}(K = 2)\ncomponents[1] (prior = 0.2000): Normal{Float64}(μ=0.0, σ=1.0)\ncomponents[2] (prior = 0.8000): Normal{Float64}(μ=0.0, σ=2.0)",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convex prior classes</span>"
    ]
  },
  {
    "objectID": "manual/flocalizations.html",
    "href": "manual/flocalizations.html",
    "title": "F-Localizations",
    "section": "",
    "text": "FLocalization",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>F-Localizations</span>"
    ]
  },
  {
    "objectID": "manual/flocalizations.html#flocalization",
    "href": "manual/flocalizations.html#flocalization",
    "title": "F-Localizations",
    "section": "",
    "text": "Abstract type representing F-Localizations.",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>F-Localizations</span>"
    ]
  },
  {
    "objectID": "manual/flocalizations.html#fittedflocalization",
    "href": "manual/flocalizations.html#fittedflocalization",
    "title": "F-Localizations",
    "section": "FittedFLocalization",
    "text": "FittedFLocalization\n\n\n\n\n\n\nAbstract type representing a fitted F-Localization (i.e., wherein the F-localization has already been determined by data).",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>F-Localizations</span>"
    ]
  },
  {
    "objectID": "manual/flocalizations.html#dvoretzkykieferwolfowitz",
    "href": "manual/flocalizations.html#dvoretzkykieferwolfowitz",
    "title": "F-Localizations",
    "section": "DvoretzkyKieferWolfowitz",
    "text": "DvoretzkyKieferWolfowitz\n\n\n\n\n\n\nDvoretzkyKieferWolfowitz(;α = 0.05, max_constraints = 1000) &lt;: FLocalization\nThe Dvoretzky-Kiefer-Wolfowitz band (based on the Kolmogorov-Smirnov distance) at confidence level 1-α that bounds the distance of the true distribution function to the ECDF \\(\\widehat{F}_n\\) based on \\(n\\) samples. The constant of the band is the sharp constant derived by Massart:\n\\[\nF \\text{ distribution}:  \\sup_{t \\in \\mathbb R}\\lvert F(t) - \\widehat{F}_n(t) \\rvert  \\leq  \\sqrt{\\log(2/\\alpha)/(2n)}\n\\]\nThe supremum above is enforced discretely on at most max_constraints number of points.",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>F-Localizations</span>"
    ]
  },
  {
    "objectID": "manual/flocalizations.html#chisquaredflocalization",
    "href": "manual/flocalizations.html#chisquaredflocalization",
    "title": "F-Localizations",
    "section": "ChiSquaredFLocalization",
    "text": "ChiSquaredFLocalization\n\n\n\n\n\n\nChiSquaredFLocalization(α) &lt;: FLocalization\nThe \\(\\chi^2\\) F-localization at confidence level \\(1-\\alpha\\) for a discrete random variable taking values in \\(\\{0,\\dotsc, N\\}\\). It is equal to:\n\\[\nf: \\sum_{x=0}^N \\frac{(n \\hat{f}_n(x) - n f(x))^2}{n f(x)} \\leq \\chi^2_{N,1-\\alpha},\n\\]\nwhere \\(\\chi^2_{N,1-\\alpha}\\) is the \\(1-\\alpha\\) quantile of the Chi-squared distribution with \\(N\\) degrees of freedom, \\(n\\) is the sample size, \\(\\hat{f}_n(x)\\) is the proportion of samples equal to \\(x\\) and \\(f(x)\\) is then population pmf.",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>F-Localizations</span>"
    ]
  },
  {
    "objectID": "manual/flocalizations.html#infinitynormdensityband",
    "href": "manual/flocalizations.html#infinitynormdensityband",
    "title": "F-Localizations",
    "section": "InfinityNormDensityBand",
    "text": "InfinityNormDensityBand\n\n\n\n\n\n\nInfinityNormDensityBand(;a_min,\n                         a_max,\n                         kernel  =  Empirikos.FlatTopKernel(),\n                         bootstrap = :Multinomial,\n                         nboot = 1000,\n                         α = 0.05,\n                         rng = Random.MersenneTwister(1)\n                    )  &lt;: FLocalization\nThis struct contains hyperparameters that will be used for constructing a neighborhood of the marginal density. The steps of the method (and corresponding hyperparameter meanings) are as follows\n\nFirst a kernel density estimate \\(\\bar{f}\\) with kernel is fit to the data.\nSecond, a bootstrap (options: :Multinomial or Poisson) with nboot bootstrap replicates will be used to estimate \\(c_n\\), such that:\n\n\\[\n\\liminf_{n \\to \\infty}\\mathbb{P}\\left[\\sup_{x \\in [a_{\\text{min}} , a_{\\text{max}}]} | \\bar{f}(x) - f(x)| \\leq c_ n\\right] \\geq 1-\\alpha\n\\]\nNote that the bound is valid from a_min to a_max. α is the nominal level and finally rng sets the seed for the bootstrap samples.\n\n\n\nThis F-Localization currently only works for homoskedastic Normal samples with common noise variance \\(\\sigma^2\\). By default the above uses the following kernel, with bandwidth \\(h = \\sigma/\\sqrt{\\log(n)}\\), where \\(n\\) is the sample size:",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>F-Localizations</span>"
    ]
  },
  {
    "objectID": "manual/flocalizations.html#flattopkernel",
    "href": "manual/flocalizations.html#flattopkernel",
    "title": "F-Localizations",
    "section": "FlatTopKernel",
    "text": "FlatTopKernel\n\n\n\n\n\n\nFlatTopKernel(h) &lt; InfiniteOrderKernel\nImplements the FlatTopKernel with bandwidth h to be used for kernel density estimation through the KernelDensity.jl package. The flat-top kernel is defined as follows:\n\\[\nK(x) = \\frac{\\sin^2(1.1x/2)-\\sin^2(x/2)}{\\pi x^2/ 20}.\n\\]\nIts use case is similar to the SincKernel, however it has the advantage of being integrable (in the Lebesgue sense) and having bounded total variation. Its Fourier transform is the following:\n\\[\nK^*(t) = \\begin{cases}\n1, & \\text{ if } t|\\leq 1 \\\\\n0, &\\text{ if } |t| \\geq 1.1 \\\\\n11-10|t|,& \\text{ if } |t| \\in [1,1.1]\n\\end{cases}\n\\]\njulia&gt; Empirikos.FlatTopKernel(0.1)\nFlatTopKernel | bandwidth = 0.1",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>F-Localizations</span>"
    ]
  },
  {
    "objectID": "manual/flocalizations.html#sinckernel",
    "href": "manual/flocalizations.html#sinckernel",
    "title": "F-Localizations",
    "section": "SincKernel",
    "text": "SincKernel\n\n\n\n\n\n\nSincKernel(h) &lt;: InfiniteOrderKernel\nImplements the SincKernel with bandwidth h to be used for kernel density estimation through the KernelDensity.jl package. The sinc kernel is defined as follows:\n\\[\nK_{\\text{sinc}}(x) = \\frac{\\sin(x)}{\\pi x}\n\\]\nIt is not typically used for kernel density estimation, because this kernel is not a density itself. However, it is particularly well suited to deconvolution problems and estimation of very smooth densities because its Fourier transform is the following:\n\\[\nK^*_{\\text{sinc}}(t) = \\mathbf 1( t \\in [-1,1])\n\\]",
    "crumbs": [
      "Manual",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>F-Localizations</span>"
    ]
  },
  {
    "objectID": "vignettes/REBayes.html",
    "href": "vignettes/REBayes.html",
    "title": "Empirikos.jl: Reproducing REBayes Examples",
    "section": "",
    "text": "Reproducing the REBayes R vignette\nThe goal of this notebook is to showcase the functionality of the Empirikos.jl package by (partially) reproducing results shown in the REBayes vignette.\nusing DocumenterQuarto\nusing Empirikos\nusing Distributions\nusing JuMP\nusing LaTeXStrings\nusing Plots\nusing StatsPlots\nusing Hypatia\nusing Random\ngr()\n\nPlots.GRBackend()",
    "crumbs": [
      "Vignettes",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Empirikos.jl: Reproducing REBayes Examples</span>"
    ]
  },
  {
    "objectID": "vignettes/REBayes.html#gaussian-mixture-models",
    "href": "vignettes/REBayes.html#gaussian-mixture-models",
    "title": "Empirikos.jl: Reproducing REBayes Examples",
    "section": "Gaussian Mixture models",
    "text": "Gaussian Mixture models\n\nNeedles and haystacks\nHere we demonstrate the classic “needles in a haystack” scenario from the REBayes vignette. This example simulates data where most observations come from a standard normal distribution (\\(\\mathrm{N}(0,1)\\)), while a small fraction (10%) come from a shifted normal (\\(\\mathrm{N}(2,1)\\)).\n\nzs = rand(MersenneTwister(100), Normal(), 1000) .+ [fill(0,900); fill(2,100)]\nzs = sort(zs);\n\nIn Empirikos.jl, we represent each observation as a StandardNormalSample object. This differs from REBayes where you would directly use a vector of observations as input to the GLmix function.\nThe StandardNormalSample type encapsulates the observation along with knowledge that it follows a standard normal likelihood (i.e., \\(Z_i \\mid \\mu_i \\sim \\mathrm{N}(\\mu_i, 1)\\)). This type-based approach is a key design feature of Empirikos.jl:\n\nIt allows the package to know the likelihood structure without additional arguments;\nIt enables multiple dispatch to handle different likelihood models;\nIt maintains a consistent interface across all empirical Bayes problems.\n\n\nnormal_Zs = StandardNormalSample.(zs)\nfirst(normal_Zs, 3)\n\n3-element Vector{StandardNormalSample{Float64}}:\n N(-3.1746277553967115; μ, σ=1.0)\n N(-2.990089450782722; μ, σ=1.0)\n N(-2.8559587291548225; μ, σ=1.0)\n\n\nWe now fit the nonparametric maximum likelihood estimator (NPMLE) using the Mosek optimizer. In REBayes, this would be done with a single call to GLmix(y), while in Empirikos we use the more general fit function with NPMLE specification:\n\nnormal_npmle = fit(NPMLE(DiscretePriorClass(), Hypatia.Optimizer), normal_Zs)\n\n\n iter        p_obj        d_obj |  abs_gap    x_feas    z_feas |      tau       kap        mu | dir_res     prox  step     alpha\n    0   3.2373e-02  -2.8080e+02 | 2.30e+03  2.05e+02  2.33e+00 | 1.00e+00  1.00e+00  1.00e+00 |\n    1  -3.9922e-01  -2.8538e+02 | 2.19e+03  2.08e+02  2.37e+00 | 9.36e-01  1.02e+00  9.50e-01 | 2.9e-12  8.7e-02  co-a  5.00e-02\n    2  -9.4365e-01  -2.8751e+02 | 2.08e+03  2.09e+02  2.38e+00 | 8.84e-01  1.02e+00  9.03e-01 | 1.3e-12  1.6e-01  co-a  5.00e-02\n    3  -2.4184e+00  -2.8975e+02 | 1.87e+03  2.10e+02  2.39e+00 | 7.93e-01  1.02e+00  8.12e-01 | 4.0e-13  4.5e-01  co-a  1.00e-01\n    4  -4.6345e+00  -2.9294e+02 | 1.68e+03  2.10e+02  2.40e+00 | 7.11e-01  1.03e+00  7.31e-01 | 3.9e-12  8.2e-02  co-a  1.00e-01\n    5  -7.7639e+00  -2.9620e+02 | 1.51e+03  2.11e+02  2.40e+00 | 6.39e-01  1.03e+00  6.58e-01 | 5.4e-12  3.3e-02  co-a  1.00e-01\n    6  -1.2275e+01  -3.0066e+02 | 1.36e+03  2.11e+02  2.40e+00 | 5.75e-01  1.03e+00  5.92e-01 | 1.6e-12  3.8e-02  co-a  1.00e-01\n    7  -1.8987e+01  -3.0723e+02 | 1.23e+03  2.11e+02  2.40e+00 | 5.17e-01  1.03e+00  5.33e-01 | 1.2e-12  5.1e-02  co-a  1.00e-01\n    8  -2.9477e+01  -3.1752e+02 | 1.10e+03  2.11e+02  2.40e+00 | 4.66e-01  1.03e+00  4.80e-01 | 2.8e-12  5.2e-02  co-a  1.00e-01\n    9  -4.5655e+01  -3.3343e+02 | 9.94e+02  2.11e+02  2.40e+00 | 4.19e-01  1.03e+00  4.32e-01 | 1.4e-11  4.8e-02  co-a  1.00e-01\n   10  -6.8694e+01  -3.5614e+02 | 8.94e+02  2.11e+02  2.40e+00 | 3.77e-01  1.03e+00  3.89e-01 | 2.8e-12  2.9e-02  co-a  1.00e-01\n   11  -9.9142e+01  -3.8621e+02 | 8.05e+02  2.11e+02  2.40e+00 | 3.40e-01  1.03e+00  3.50e-01 | 1.3e-11  2.6e-02  co-a  1.00e-01\n   12  -1.3729e+02  -4.2390e+02 | 7.24e+02  2.11e+02  2.40e+00 | 3.06e-01  1.03e+00  3.15e-01 | 1.2e-12  2.1e-02  co-a  1.00e-01\n   13  -3.2935e+02  -6.1386e+02 | 5.07e+02  2.10e+02  2.39e+00 | 2.15e-01  1.03e+00  2.20e-01 | 2.5e-12  9.5e-01  co-a  3.00e-01\n   14  -6.3843e+02  -9.1902e+02 | 3.55e+02  2.09e+02  2.38e+00 | 1.51e-01  1.02e+00  1.54e-01 | 3.2e-12  2.4e-01  co-a  3.00e-01\n   15  -7.5222e+02  -1.0296e+03 | 3.19e+02  2.07e+02  2.36e+00 | 1.37e-01  1.01e+00  1.39e-01 | 2.1e-11  4.5e-02  co-a  1.00e-01\n   16  -8.7220e+02  -1.1447e+03 | 2.88e+02  2.04e+02  2.32e+00 | 1.26e-01  9.96e-01  1.25e-01 | 2.7e-11  7.5e-02  co-a  1.00e-01\n   17  -9.6568e+02  -1.2242e+03 | 2.59e+02  1.93e+02  2.20e+00 | 1.19e-01  9.47e-01  1.12e-01 | 2.1e-11  2.9e-01  co-a  1.00e-01\n   18  -7.8195e+02  -9.7089e+02 | 2.33e+02  1.40e+02  1.60e+00 | 1.48e-01  6.35e-01  1.01e-01 | 4.8e-11  1.2e-01  co-a  1.00e-01\n   19  -6.0313e+02  -7.3789e+02 | 2.10e+02  9.97e+01  1.14e+00 | 1.87e-01  4.87e-01  9.11e-02 | 4.2e-11  7.9e-01  co-a  1.00e-01\n   20  -4.9603e+02  -5.9886e+02 | 1.89e+02  7.59e+01  8.64e-01 | 2.21e-01  3.73e-01  8.20e-02 | 3.1e-11  4.2e-01  co-a  1.00e-01\n   21  -4.2283e+02  -5.0496e+02 | 1.70e+02  6.05e+01  6.89e-01 | 2.49e-01  2.99e-01  7.38e-02 | 4.1e-11  1.3e-01  co-a  1.00e-01\n   22  -2.7948e+02  -3.2455e+02 | 1.19e+02  3.31e+01  3.77e-01 | 3.19e-01  1.81e-01  5.17e-02 | 2.4e-11  1.2e-01  co-a  3.00e-01\n   23  -2.0630e+02  -2.3488e+02 | 8.33e+01  2.10e+01  2.39e-01 | 3.53e-01  1.05e-01  3.62e-02 | 2.2e-11  6.6e-01  co-a  3.00e-01\n   24  -2.0088e+02  -2.2838e+02 | 7.91e+01  2.02e+01  2.30e-01 | 3.48e-01  9.87e-02  3.44e-02 | 6.2e-11  5.7e-01  co-a  5.00e-02\n   25  -1.8483e+02  -2.0908e+02 | 7.12e+01  1.78e+01  2.03e-01 | 3.56e-01  8.69e-02  3.09e-02 | 1.4e-11  3.8e-01  co-a  1.00e-01\n   26  -1.7155e+02  -1.9323e+02 | 6.40e+01  1.59e+01  1.81e-01 | 3.58e-01  7.77e-02  2.78e-02 | 1.5e-11  3.5e-01  co-a  1.00e-01\n   27  -1.5902e+02  -1.7835e+02 | 5.76e+01  1.42e+01  1.61e-01 | 3.61e-01  6.93e-02  2.50e-02 | 1.0e-11  2.5e-01  co-a  1.00e-01\n   28  -1.4812e+02  -1.6550e+02 | 5.18e+01  1.27e+01  1.45e-01 | 3.62e-01  6.23e-02  2.25e-02 | 8.2e-12  2.2e-01  co-a  1.00e-01\n   29  -1.3792e+02  -1.5353e+02 | 4.66e+01  1.14e+01  1.30e-01 | 3.62e-01  5.59e-02  2.03e-02 | 1.3e-11  1.7e-01  co-a  1.00e-01\n   30  -1.2878e+02  -1.4288e+02 | 4.20e+01  1.03e+01  1.18e-01 | 3.61e-01  5.05e-02  1.82e-02 | 1.0e-11  1.5e-01  co-a  1.00e-01\n   31  -1.2024e+02  -1.3297e+02 | 3.78e+01  9.33e+00  1.06e-01 | 3.60e-01  4.56e-02  1.64e-02 | 9.2e-12  1.3e-01  co-a  1.00e-01\n   32  -1.1245e+02  -1.2397e+02 | 3.40e+01  8.45e+00  9.62e-02 | 3.58e-01  4.13e-02  1.48e-02 | 5.9e-12  1.2e-01  co-a  1.00e-01\n   33  -1.0516e+02  -1.1560e+02 | 3.06e+01  7.65e+00  8.72e-02 | 3.56e-01  3.74e-02  1.33e-02 | 6.0e-12  1.1e-01  co-a  1.00e-01\n   34  -9.8435e+01  -1.0790e+02 | 2.75e+01  6.94e+00  7.91e-02 | 3.53e-01  3.39e-02  1.20e-02 | 7.2e-12  1.0e-01  co-a  1.00e-01\n   35  -9.2138e+01  -1.0073e+02 | 2.48e+01  6.30e+00  7.18e-02 | 3.50e-01  3.08e-02  1.08e-02 | 9.4e-12  9.9e-02  co-a  1.00e-01\n   36  -8.6285e+01  -9.4088e+01 | 2.23e+01  5.72e+00  6.52e-02 | 3.47e-01  2.79e-02  9.68e-03 | 5.4e-12  9.8e-02  co-a  1.00e-01\n   37  -8.0800e+01  -8.7890e+01 | 2.01e+01  5.20e+00  5.92e-02 | 3.43e-01  2.54e-02  8.72e-03 | 6.4e-12  9.7e-02  co-a  1.00e-01\n   38  -7.5680e+01  -8.2125e+01 | 1.80e+01  4.73e+00  5.39e-02 | 3.40e-01  2.31e-02  7.84e-03 | 5.1e-12  9.7e-02  co-a  1.00e-01\n   39  -7.0878e+01  -7.6738e+01 | 1.62e+01  4.30e+00  4.90e-02 | 3.36e-01  2.10e-02  7.06e-03 | 5.8e-12  9.7e-02  co-a  1.00e-01\n   40  -6.6385e+01  -7.1714e+01 | 1.46e+01  3.91e+00  4.45e-02 | 3.33e-01  1.91e-02  6.35e-03 | 5.7e-12  9.7e-02  co-a  1.00e-01\n   41  -6.2169e+01  -6.7015e+01 | 1.32e+01  3.56e+00  4.05e-02 | 3.29e-01  1.74e-02  5.72e-03 | 5.4e-12  9.7e-02  co-a  1.00e-01\n   42  -5.8219e+01  -6.2626e+01 | 1.18e+01  3.23e+00  3.68e-02 | 3.26e-01  1.58e-02  5.14e-03 | 2.9e-12  9.8e-02  co-a  1.00e-01\n   43  -5.4513e+01  -5.8519e+01 | 1.07e+01  2.94e+00  3.35e-02 | 3.23e-01  1.43e-02  4.63e-03 | 3.1e-12  9.8e-02  co-a  1.00e-01\n   44  -5.1037e+01  -5.4679e+01 | 9.59e+00  2.67e+00  3.05e-02 | 3.19e-01  1.30e-02  4.17e-03 | 2.9e-12  9.9e-02  co-a  1.00e-01\n   45  -4.7777e+01  -5.1087e+01 | 8.63e+00  2.43e+00  2.77e-02 | 3.16e-01  1.19e-02  3.75e-03 | 2.6e-12  9.9e-02  co-a  1.00e-01\n   46  -4.4720e+01  -4.7727e+01 | 7.76e+00  2.21e+00  2.52e-02 | 3.13e-01  1.08e-02  3.37e-03 | 2.2e-12  1.0e-01  co-a  1.00e-01\n   47  -4.1854e+01  -4.4584e+01 | 6.99e+00  2.01e+00  2.28e-02 | 3.10e-01  9.78e-03  3.04e-03 | 3.3e-12  1.0e-01  co-a  1.00e-01\n   48  -3.9167e+01  -4.1646e+01 | 6.29e+00  1.82e+00  2.07e-02 | 3.08e-01  8.87e-03  2.73e-03 | 2.5e-12  1.0e-01  co-a  1.00e-01\n   49  -3.6650e+01  -3.8898e+01 | 5.66e+00  1.65e+00  1.88e-02 | 3.05e-01  8.05e-03  2.46e-03 | 1.6e-12  1.0e-01  co-a  1.00e-01\n   50  -3.4293e+01  -3.6331e+01 | 5.09e+00  1.50e+00  1.71e-02 | 3.03e-01  7.30e-03  2.21e-03 | 2.6e-12  1.0e-01  co-a  1.00e-01\n   51  -3.2087e+01  -3.3933e+01 | 4.58e+00  1.36e+00  1.54e-02 | 3.01e-01  6.61e-03  1.99e-03 | 1.4e-12  1.0e-01  co-a  1.00e-01\n   52  -3.0023e+01  -3.1694e+01 | 4.12e+00  1.23e+00  1.40e-02 | 2.99e-01  5.98e-03  1.79e-03 | 1.9e-12  1.0e-01  co-a  1.00e-01\n   53  -2.8094e+01  -2.9606e+01 | 3.71e+00  1.11e+00  1.26e-02 | 2.98e-01  5.41e-03  1.61e-03 | 1.3e-12  1.0e-01  co-a  1.00e-01\n   54  -2.6293e+01  -2.7658e+01 | 3.34e+00  1.00e+00  1.14e-02 | 2.97e-01  4.89e-03  1.45e-03 | 1.1e-12  1.0e-01  co-a  1.00e-01\n   55  -2.4612e+01  -2.5844e+01 | 3.00e+00  9.06e-01  1.03e-02 | 2.96e-01  4.41e-03  1.31e-03 | 1.2e-12  1.0e-01  co-a  1.00e-01\n   56  -2.3044e+01  -2.4155e+01 | 2.70e+00  8.17e-01  9.30e-03 | 2.95e-01  3.98e-03  1.18e-03 | 1.2e-12  1.0e-01  co-a  1.00e-01\n   57  -2.1583e+01  -2.2584e+01 | 2.43e+00  7.36e-01  8.38e-03 | 2.95e-01  3.58e-03  1.06e-03 | 9.9e-13  1.0e-01  co-a  1.00e-01\n   58  -2.0223e+01  -2.1123e+01 | 2.19e+00  6.62e-01  7.54e-03 | 2.95e-01  3.22e-03  9.52e-04 | 9.8e-13  1.1e-01  co-a  1.00e-01\n   59  -1.8957e+01  -1.9765e+01 | 1.97e+00  5.94e-01  6.77e-03 | 2.96e-01  2.89e-03  8.56e-04 | 1.1e-12  1.1e-01  co-a  1.00e-01\n   60  -1.7779e+01  -1.8503e+01 | 1.77e+00  5.33e-01  6.07e-03 | 2.97e-01  2.59e-03  7.71e-04 | 7.5e-13  1.1e-01  co-a  1.00e-01\n   61  -1.6684e+01  -1.7333e+01 | 1.60e+00  4.77e-01  5.43e-03 | 2.99e-01  2.32e-03  6.93e-04 | 7.7e-13  1.1e-01  co-a  1.00e-01\n   62  -1.5671e+01  -1.6251e+01 | 1.44e+00  4.26e-01  4.85e-03 | 3.01e-01  2.07e-03  6.24e-04 | 1.0e-12  1.0e-01  co-a  1.00e-01\n   63  -1.4738e+01  -1.5255e+01 | 1.29e+00  3.80e-01  4.33e-03 | 3.04e-01  1.85e-03  5.62e-04 | 2.0e-12  9.5e-02  co-a  1.00e-01\n   64  -1.3883e+01  -1.4344e+01 | 1.16e+00  3.38e-01  3.85e-03 | 3.07e-01  1.65e-03  5.05e-04 | 1.1e-12  8.7e-02  co-a  1.00e-01\n   65  -1.3103e+01  -1.3513e+01 | 1.05e+00  3.01e-01  3.43e-03 | 3.10e-01  1.47e-03  4.55e-04 | 3.1e-12  7.8e-02  co-a  1.00e-01\n   66  -1.2395e+01  -1.2759e+01 | 9.42e-01  2.68e-01  3.05e-03 | 3.14e-01  1.30e-03  4.09e-04 | 1.6e-12  6.9e-02  co-a  1.00e-01\n   67  -1.1753e+01  -1.2077e+01 | 8.48e-01  2.38e-01  2.71e-03 | 3.18e-01  1.16e-03  3.68e-04 | 9.5e-13  6.1e-02  co-a  1.00e-01\n   68  -1.0024e+01  -1.0242e+01 | 5.93e-01  1.60e-01  1.82e-03 | 3.31e-01  7.76e-04  2.58e-04 | 1.5e-12  9.4e-01  co-a  3.00e-01\n   69  -9.9984e+00  -1.0215e+01 | 5.87e-01  1.59e-01  1.81e-03 | 3.30e-01  7.73e-04  2.55e-04 | 2.9e-10  8.7e-01  co-a  1.00e-02\n   70  -9.9475e+00  -1.0161e+01 | 5.81e-01  1.57e-01  1.79e-03 | 3.31e-01  7.62e-04  2.52e-04 | 3.0e-11  5.6e-01  co-a  1.00e-02\n   71  -9.5661e+00  -9.7569e+00 | 5.23e-01  1.40e-01  1.60e-03 | 3.34e-01  6.81e-04  2.27e-04 | 6.4e-12  8.7e-01  co-a  1.00e-01\n   72  -8.5075e+00  -8.6359e+00 | 3.65e-01  9.42e-02  1.07e-03 | 3.47e-01  4.56e-04  1.59e-04 | 2.1e-12  9.1e-01  co-a  3.00e-01\n   73  -8.4833e+00  -8.6103e+00 | 3.61e-01  9.32e-02  1.06e-03 | 3.48e-01  4.52e-04  1.57e-04 | 1.5e-10  7.3e-01  co-a  1.00e-02\n   74  -8.3679e+00  -8.4882e+00 | 3.43e-01  8.83e-02  1.01e-03 | 3.49e-01  4.28e-04  1.49e-04 | 1.2e-11  7.4e-01  co-a  5.00e-02\n   75  -7.7036e+00  -7.7856e+00 | 2.40e-01  6.01e-02  6.85e-04 | 3.58e-01  2.91e-04  1.04e-04 | 3.6e-12  9.6e-01  co-a  3.00e-01\n   76  -7.6274e+00  -7.7050e+00 | 2.27e-01  5.69e-02  6.48e-04 | 3.60e-01  2.75e-04  9.88e-05 | 2.0e-11  7.4e-01  co-a  5.00e-02\n   77  -7.4831e+00  -7.5524e+00 | 2.05e-01  5.08e-02  5.79e-04 | 3.63e-01  2.45e-04  8.89e-05 | 1.8e-12  4.2e-01  co-a  1.00e-01\n   78  -7.1040e+00  -7.1517e+00 | 1.43e-01  3.49e-02  3.98e-04 | 3.69e-01  1.68e-04  6.22e-05 | 4.5e-12  6.4e-01  co-a  3.00e-01\n   79  -7.0185e+00  -7.0613e+00 | 1.29e-01  3.14e-02  3.57e-04 | 3.70e-01  1.51e-04  5.59e-05 | 7.2e-12  9.4e-01  co-a  1.00e-01\n   80  -6.7875e+00  -6.8171e+00 | 8.99e-02  2.17e-02  2.47e-04 | 3.74e-01  1.05e-04  3.91e-05 | 4.0e-12  9.2e-01  co-a  3.00e-01\n   81  -6.7823e+00  -6.8116e+00 | 8.90e-02  2.15e-02  2.45e-04 | 3.75e-01  1.03e-04  3.87e-05 | 1.8e-10  7.5e-01  co-a  1.00e-02\n   82  -6.7563e+00  -6.7841e+00 | 8.45e-02  2.04e-02  2.32e-04 | 3.75e-01  9.81e-05  3.67e-05 | 2.0e-11  6.5e-01  co-a  5.00e-02\n   83  -6.6072e+00  -6.6265e+00 | 5.91e-02  1.42e-02  1.61e-04 | 3.77e-01  6.81e-05  2.57e-05 | 5.9e-12  9.2e-01  co-a  3.00e-01\n   84  -6.5902e+00  -6.6085e+00 | 5.61e-02  1.35e-02  1.53e-04 | 3.77e-01  6.47e-05  2.44e-05 | 5.5e-11  9.5e-01  co-a  5.00e-02\n   85  -6.5575e+00  -6.5740e+00 | 5.05e-02  1.21e-02  1.38e-04 | 3.78e-01  5.80e-05  2.19e-05 | 1.9e-11  7.2e-01  co-a  1.00e-01\n   86  -6.5284e+00  -6.5432e+00 | 4.54e-02  1.09e-02  1.24e-04 | 3.79e-01  5.21e-05  1.97e-05 | 3.9e-12  3.6e-01  co-a  1.00e-01\n   87  -6.4494e+00  -6.4598e+00 | 3.18e-02  7.57e-03  8.62e-05 | 3.80e-01  3.63e-05  1.38e-05 | 3.8e-12  7.8e-01  co-a  3.00e-01\n   88  -6.4403e+00  -6.4501e+00 | 3.02e-02  7.19e-03  8.19e-05 | 3.80e-01  3.45e-05  1.31e-05 | 2.8e-11  6.9e-01  co-a  5.00e-02\n   89  -6.4229e+00  -6.4317e+00 | 2.72e-02  6.47e-03  7.37e-05 | 3.81e-01  3.10e-05  1.18e-05 | 4.8e-12  4.6e-01  co-a  1.00e-01\n   90  -6.3760e+00  -6.3822e+00 | 1.90e-02  4.52e-03  5.14e-05 | 3.82e-01  2.16e-05  8.26e-06 | 2.5e-12  8.8e-01  co-a  3.00e-01\n   91  -6.3705e+00  -6.3764e+00 | 1.80e-02  4.29e-03  4.89e-05 | 3.82e-01  2.06e-05  7.84e-06 | 1.5e-10  9.0e-01  co-a  5.00e-02\n   92  -6.3601e+00  -6.3654e+00 | 1.62e-02  3.86e-03  4.40e-05 | 3.82e-01  1.85e-05  7.05e-06 | 1.8e-11  7.8e-01  co-a  1.00e-01\n   93  -6.3509e+00  -6.3556e+00 | 1.46e-02  3.47e-03  3.95e-05 | 3.82e-01  1.66e-05  6.35e-06 | 5.3e-12  4.2e-01  co-a  1.00e-01\n   94  -6.3256e+00  -6.3290e+00 | 1.02e-02  2.43e-03  2.76e-05 | 3.83e-01  1.16e-05  4.44e-06 | 3.8e-12  8.2e-01  co-a  3.00e-01\n   95  -6.3197e+00  -6.3227e+00 | 9.19e-03  2.18e-03  2.49e-05 | 3.83e-01  1.04e-05  3.99e-06 | 2.5e-11  9.5e-01  co-a  1.00e-01\n   96  -6.3145e+00  -6.3172e+00 | 8.27e-03  1.96e-03  2.24e-05 | 3.83e-01  9.39e-06  3.59e-06 | 6.2e-12  5.6e-01  co-a  1.00e-01\n   97  -6.3002e+00  -6.3021e+00 | 5.78e-03  1.37e-03  1.57e-05 | 3.83e-01  6.57e-06  2.51e-06 | 5.2e-12  9.4e-01  co-a  3.00e-01\n   98  -6.2986e+00  -6.3003e+00 | 5.49e-03  1.31e-03  1.49e-05 | 3.83e-01  6.24e-06  2.39e-06 | 5.8e-11  8.5e-01  co-a  5.00e-02\n   99  -6.2954e+00  -6.2970e+00 | 4.94e-03  1.18e-03  1.34e-05 | 3.83e-01  5.61e-06  2.15e-06 | 9.9e-12  7.0e-01  co-a  1.00e-01\n  100  -6.2926e+00  -6.2940e+00 | 4.45e-03  1.06e-03  1.20e-05 | 3.83e-01  5.05e-06  1.93e-06 | 8.1e-12  4.0e-01  co-a  1.00e-01\n  101  -6.2849e+00  -6.2859e+00 | 3.11e-03  7.40e-04  8.43e-06 | 3.83e-01  3.53e-06  1.35e-06 | 5.2e-12  7.4e-01  co-a  3.00e-01\n  102  -6.2831e+00  -6.2840e+00 | 2.80e-03  6.66e-04  7.59e-06 | 3.83e-01  3.18e-06  1.22e-06 | 1.7e-11  6.9e-01  co-a  1.00e-01\n  103  -6.2783e+00  -6.2789e+00 | 1.96e-03  4.66e-04  5.31e-06 | 3.83e-01  2.22e-06  8.51e-07 | 4.8e-12  9.8e-01  co-a  3.00e-01\n  104  -6.2777e+00  -6.2783e+00 | 1.86e-03  4.43e-04  5.05e-06 | 3.83e-01  2.11e-06  8.08e-07 | 1.3e-10  9.2e-01  co-a  5.00e-02\n  105  -6.2766e+00  -6.2772e+00 | 1.67e-03  3.99e-04  4.54e-06 | 3.83e-01  1.90e-06  7.27e-07 | 9.7e-11  8.8e-01  co-a  1.00e-01\n  106  -6.2757e+00  -6.2762e+00 | 1.50e-03  3.59e-04  4.09e-06 | 3.83e-01  1.71e-06  6.54e-07 | 1.7e-11  5.2e-01  co-a  1.00e-01\n  107  -6.2731e+00  -6.2734e+00 | 1.05e-03  2.51e-04  2.86e-06 | 3.83e-01  1.20e-06  4.58e-07 | 7.9e-12  7.9e-01  co-a  3.00e-01\n  108  -6.2725e+00  -6.2728e+00 | 9.47e-04  2.26e-04  2.58e-06 | 3.83e-01  1.08e-06  4.12e-07 | 6.2e-11  6.9e-01  co-a  1.00e-01\n  109  -6.2708e+00  -6.2711e+00 | 6.63e-04  1.58e-04  1.80e-06 | 3.83e-01  7.53e-07  2.88e-07 | 2.5e-11  8.7e-01  co-a  3.00e-01\n  110  -6.2705e+00  -6.2707e+00 | 5.96e-04  1.43e-04  1.62e-06 | 3.83e-01  6.77e-07  2.59e-07 | 1.7e-10  8.6e-01  co-a  1.00e-01\n  111  -6.2701e+00  -6.2703e+00 | 5.36e-04  1.28e-04  1.46e-06 | 3.83e-01  6.10e-07  2.33e-07 | 6.1e-11  4.3e-01  co-a  1.00e-01\n  112  -6.2692e+00  -6.2693e+00 | 3.75e-04  8.99e-05  1.02e-06 | 3.82e-01  4.27e-07  1.63e-07 | 2.4e-11  6.0e-01  co-a  3.00e-01\n  113  -6.2690e+00  -6.2691e+00 | 3.38e-04  8.09e-05  9.22e-07 | 3.82e-01  3.84e-07  1.47e-07 | 4.1e-11  4.4e-01  co-a  1.00e-01\n  114  -6.2684e+00  -6.2685e+00 | 2.36e-04  5.66e-05  6.45e-07 | 3.82e-01  2.69e-07  1.03e-07 | 1.1e-11  5.5e-01  co-a  3.00e-01\n  115  -6.2683e+00  -6.2684e+00 | 2.13e-04  5.10e-05  5.81e-07 | 3.82e-01  2.42e-07  9.24e-08 | 1.2e-10  4.2e-01  co-a  1.00e-01\n  116  -6.2679e+00  -6.2680e+00 | 1.49e-04  3.57e-05  4.07e-07 | 3.82e-01  1.69e-07  6.47e-08 | 2.0e-11  5.1e-01  co-a  3.00e-01\n  117  -6.2677e+00  -6.2677e+00 | 1.04e-04  2.50e-05  2.85e-07 | 3.82e-01  1.19e-07  4.52e-08 | 2.0e-10  9.7e-01  co-a  3.00e-01\n  118  -6.2676e+00  -6.2677e+00 | 9.37e-05  2.25e-05  2.56e-07 | 3.82e-01  1.07e-07  4.07e-08 | 3.5e-10  5.0e-01  co-a  1.00e-01\n  119  -6.2675e+00  -6.2675e+00 | 6.55e-05  1.58e-05  1.80e-07 | 3.82e-01  7.47e-08  2.85e-08 | 7.2e-11  5.6e-01  co-a  3.00e-01\n  120  -6.2674e+00  -6.2674e+00 | 4.58e-05  1.10e-05  1.26e-07 | 3.81e-01  5.23e-08  1.99e-08 | 3.5e-10  8.6e-01  co-a  3.00e-01\n  121  -6.2673e+00  -6.2673e+00 | 3.21e-05  7.74e-06  8.81e-08 | 3.81e-01  3.66e-08  1.39e-08 | 6.7e-10  9.4e-01  co-a  3.00e-01\n  122  -6.2673e+00  -6.2673e+00 | 2.88e-05  6.97e-06  7.94e-08 | 3.81e-01  3.29e-08  1.25e-08 | 1.8e-09  5.4e-01  co-a  1.00e-01\n  123  -6.2672e+00  -6.2672e+00 | 2.02e-05  4.88e-06  5.56e-08 | 3.81e-01  2.30e-08  8.77e-09 | 3.8e-10  4.7e-01  co-a  3.00e-01\n  124  -6.2672e+00  -6.2672e+00 | 1.41e-05  3.42e-06  3.89e-08 | 3.81e-01  1.61e-08  6.14e-09 | 3.1e-10  5.0e-01  co-a  3.00e-01\n  125  -6.2672e+00  -6.2672e+00 | 9.88e-06  2.39e-06  2.72e-08 | 3.80e-01  1.13e-08  4.29e-09 | 2.8e-10  4.0e-01  co-a  3.00e-01\n  126  -6.2672e+00  -6.2672e+00 | 6.91e-06  1.67e-06  1.91e-08 | 3.80e-01  7.90e-09  3.00e-09 | 2.5e-10  3.3e-01  co-a  3.00e-01\n  127  -6.2672e+00  -6.2672e+00 | 4.84e-06  1.17e-06  1.34e-08 | 3.80e-01  5.53e-09  2.10e-09 | 2.5e-10  2.3e-01  co-a  3.00e-01\n  128  -6.2671e+00  -6.2671e+00 | 2.42e-06  5.86e-07  6.68e-09 | 3.80e-01  2.77e-09  1.05e-09 | 1.4e-10  8.9e-01  co-a  5.00e-01\n  129  -6.2671e+00  -6.2671e+00 | 2.18e-06  5.28e-07  6.01e-09 | 3.80e-01  2.49e-09  9.46e-10 | 2.1e-08  4.8e-01  co-a  1.00e-01\n  130  -6.2671e+00  -6.2671e+00 | 1.52e-06  3.70e-07  4.21e-09 | 3.80e-01  1.74e-09  6.62e-10 | 2.5e-10  3.3e-01  co-a  3.00e-01\n  131  -6.2671e+00  -6.2671e+00 | 1.37e-06  3.33e-07  3.79e-09 | 3.80e-01  1.57e-09  5.96e-10 | 3.3e-10  3.8e-02  co-a  1.00e-01\n  132  -6.2671e+00  -6.2671e+00 | 1.23e-06  2.99e-07  3.41e-09 | 3.80e-01  1.41e-09  5.36e-10 | 3.7e-10  6.3e-03  co-a  1.00e-01\n  133  -6.2671e+00  -6.2671e+00 | 6.17e-07  1.50e-07  1.71e-09 | 3.80e-01  7.07e-10  2.68e-10 | 1.6e-08  5.8e-01  co-a  5.00e-01\n  134  -6.2671e+00  -6.2671e+00 | 3.09e-07  7.50e-08  8.54e-10 | 3.80e-01  3.54e-10  1.34e-10 | 3.5e-10  5.9e-01  co-a  5.00e-01\n  135  -6.2671e+00  -6.2671e+00 | 2.78e-07  6.75e-08  7.68e-10 | 3.80e-01  3.18e-10  1.21e-10 | 3.4e-10  7.9e-02  co-a  1.00e-01\n  136  -6.2671e+00  -6.2671e+00 | 1.94e-07  4.72e-08  5.38e-10 | 3.79e-01  2.23e-10  8.45e-11 | 2.3e-09  1.9e-01  co-a  3.00e-01\n  137  -6.2671e+00  -6.2671e+00 | 1.75e-07  4.25e-08  4.84e-10 | 3.80e-01  2.00e-10  7.61e-11 | 9.5e-10  3.4e-02  co-a  1.00e-01\n  138  -6.2671e+00  -6.2671e+00 | 1.22e-07  2.98e-08  3.39e-10 | 3.79e-01  1.40e-10  5.32e-11 | 1.3e-09  1.6e-01  co-a  3.00e-01\n  139  -6.2671e+00  -6.2671e+00 | 1.21e-07  2.95e-08  3.36e-10 | 3.79e-01  1.39e-10  5.27e-11 | 7.8e-10  3.3e-03  co-a  1.00e-02\n  140  -6.2671e+00  -6.2671e+00 | 1.09e-07  2.65e-08  3.02e-10 | 3.79e-01  1.25e-10  4.74e-11 | 7.9e-11  5.0e-03  co-a  1.00e-01\n  141  -6.2671e+00  -6.2671e+00 | 9.82e-08  2.39e-08  2.72e-10 | 3.79e-01  1.12e-10  4.27e-11 | 2.3e-10  2.8e-03  co-a  1.00e-01\n  142  -6.2671e+00  -6.2671e+00 | 3.93e-08  9.55e-09  1.09e-10 | 3.79e-01  4.50e-11  1.71e-11 | 7.9e-12  5.2e-01  co-a  6.00e-01\noptimal solution found; terminating\n\nstatus is Optimal after 142 iterations and 20.882 seconds\n\n\n\nFitted NPMLE with Hypatia.Optimizer and 𝒢:\nDiscretePriorClass | support = -3.1747277553967117:0.030043037229013896:5.808140376078443\n\n\nThe NPMLE estimates the mixing distribution nonparametrically, which in this case should identify two mass points - one near 0 and another near 2.\nNext, we define our posterior targets - in this case, the posterior means:\n\nnormal_postmean_targets = PosteriorMean.(normal_Zs)\n\n1000-element Vector{PosteriorMean{StandardNormalSample{Float64}}}:\n 𝔼[μ | N(-3.1746277553967115; μ, σ=1.0)]\n 𝔼[μ | N(-2.990089450782722; μ, σ=1.0)]\n 𝔼[μ | N(-2.8559587291548225; μ, σ=1.0)]\n 𝔼[μ | N(-2.7380023441265133; μ, σ=1.0)]\n 𝔼[μ | N(-2.724951899176289; μ, σ=1.0)]\n 𝔼[μ | N(-2.6950346914636967; μ, σ=1.0)]\n 𝔼[μ | N(-2.5584323721075397; μ, σ=1.0)]\n 𝔼[μ | N(-2.5305668218766053; μ, σ=1.0)]\n 𝔼[μ | N(-2.3632683788658877; μ, σ=1.0)]\n 𝔼[μ | N(-2.361941909453861; μ, σ=1.0)]\n 𝔼[μ | N(-2.355281185624943; μ, σ=1.0)]\n 𝔼[μ | N(-2.342143690964997; μ, σ=1.0)]\n 𝔼[μ | N(-2.284751655993758; μ, σ=1.0)]\n ⋮\n 𝔼[μ | N(3.1330895294747294; μ, σ=1.0)]\n 𝔼[μ | N(3.371564222667284; μ, σ=1.0)]\n 𝔼[μ | N(3.3919915435058083; μ, σ=1.0)]\n 𝔼[μ | N(3.4247099212446166; μ, σ=1.0)]\n 𝔼[μ | N(3.42836573942505; μ, σ=1.0)]\n 𝔼[μ | N(3.835965091861546; μ, σ=1.0)]\n 𝔼[μ | N(4.011422275963797; μ, σ=1.0)]\n 𝔼[μ | N(4.129379367933984; μ, σ=1.0)]\n 𝔼[μ | N(4.1511807079199565; μ, σ=1.0)]\n 𝔼[μ | N(4.46857344617616; μ, σ=1.0)]\n 𝔼[μ | N(4.538449778609994; μ, σ=1.0)]\n 𝔼[μ | N(5.808040376078443; μ, σ=1.0)]\n\n\nThe following code creates a three-panel plot showing: 1. The true mixture density 2. The estimated prior distribution (mixing distribution) 3. The posterior mean function (the shrinkage estimator)\n\nnormal_pl_marginal = plot(marginalize(NormalSample(1.0),                          \n                 DiscreteNonParametric([0.0;2.0], [0.9;0.1])),                                    \n                 components=false, label=nothing, xguide=\"z\",\n                 yguide = \"f(z)\",                         \n                 title = \"True mixture\")\n\nnormal_pl_npmle_prior = plot(support(normal_npmle.prior),                            \n                 probs(normal_npmle.prior), seriestype=:sticks,                              \n                 xguide = L\"\\mu\", yguide=L\"g(\\mu)\",                              \n                 title = \"NPMLE prior estimate\", label=nothing)\n\nnormal_pl_posterior_mean = plot(response.(location.(normal_postmean_targets)),                               \n                 normal_postmean_targets.(normal_npmle.prior),                               \n                 label=nothing, title=\"NPMLE posterior mean\",                                \n                 xguide=\"z\", yguide=L\"E[\\mu \\mid Z=z]\")\n\nplot(normal_pl_marginal, normal_pl_npmle_prior, normal_pl_posterior_mean,    \n                 size=(1000,350), layout=(1,3))\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.1: Gaussian mixture model analysis. Left: True mixture density. Middle: NPMLE estimated prior. Right: Posterior mean function showing shrinkage effect.\n\n\n\n\nThis plot is similar to what you would see in REBayes but is generated using Empirikos.jl’s object-oriented approach. The posterior mean function (right panel) shows the shrinkage effect: observations are pulled toward the mass points of the estimated prior distribution. The shrinkage is stronger for values close to the mass points and weaker in regions between them.",
    "crumbs": [
      "Vignettes",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Empirikos.jl: Reproducing REBayes Examples</span>"
    ]
  },
  {
    "objectID": "vignettes/REBayes.html#binomial-npmle",
    "href": "vignettes/REBayes.html#binomial-npmle",
    "title": "Empirikos.jl: Reproducing REBayes Examples",
    "section": "Binomial NPMLE",
    "text": "Binomial NPMLE\nNow we’ll analyze the Tacks dataset using a binomial mixture model. This classic dataset from Beckett and Diaconis involves outcomes from tossing thumbtacks.\n\ntacks_tbl = Empirikos.Tacks.load_table()\n\n320-element CSV.File:\n CSV.Row: (x = 7, k = 9)\n CSV.Row: (x = 4, k = 9)\n CSV.Row: (x = 6, k = 9)\n CSV.Row: (x = 6, k = 9)\n CSV.Row: (x = 6, k = 9)\n CSV.Row: (x = 6, k = 9)\n CSV.Row: (x = 8, k = 9)\n CSV.Row: (x = 6, k = 9)\n CSV.Row: (x = 5, k = 9)\n CSV.Row: (x = 8, k = 9)\n CSV.Row: (x = 6, k = 9)\n CSV.Row: (x = 3, k = 9)\n CSV.Row: (x = 3, k = 9)\n ⋮\n CSV.Row: (x = 8, k = 9)\n CSV.Row: (x = 7, k = 9)\n CSV.Row: (x = 6, k = 9)\n CSV.Row: (x = 6, k = 9)\n CSV.Row: (x = 8, k = 9)\n CSV.Row: (x = 6, k = 9)\n CSV.Row: (x = 9, k = 9)\n CSV.Row: (x = 6, k = 9)\n CSV.Row: (x = 7, k = 9)\n CSV.Row: (x = 6, k = 9)\n CSV.Row: (x = 6, k = 9)\n CSV.Row: (x = 6, k = 9)\n\n\nWe convert the table to a vector of BinomialSample objects, where each sample represents the number of successes out of a known number of trials:\n\ntack_Zs = BinomialSample.(tacks_tbl.x, Int64.(tacks_tbl.k))\n\n320-element Vector{BinomialSample{Int64, Int64}}:\n ℬ𝒾𝓃(7; p, n=9)\n ℬ𝒾𝓃(4; p, n=9)\n ℬ𝒾𝓃(6; p, n=9)\n ℬ𝒾𝓃(6; p, n=9)\n ℬ𝒾𝓃(6; p, n=9)\n ℬ𝒾𝓃(6; p, n=9)\n ℬ𝒾𝓃(8; p, n=9)\n ℬ𝒾𝓃(6; p, n=9)\n ℬ𝒾𝓃(5; p, n=9)\n ℬ𝒾𝓃(8; p, n=9)\n ℬ𝒾𝓃(6; p, n=9)\n ℬ𝒾𝓃(3; p, n=9)\n ℬ𝒾𝓃(3; p, n=9)\n ⋮\n ℬ𝒾𝓃(8; p, n=9)\n ℬ𝒾𝓃(7; p, n=9)\n ℬ𝒾𝓃(6; p, n=9)\n ℬ𝒾𝓃(6; p, n=9)\n ℬ𝒾𝓃(8; p, n=9)\n ℬ𝒾𝓃(6; p, n=9)\n ℬ𝒾𝓃(9; p, n=9)\n ℬ𝒾𝓃(6; p, n=9)\n ℬ𝒾𝓃(7; p, n=9)\n ℬ𝒾𝓃(6; p, n=9)\n ℬ𝒾𝓃(6; p, n=9)\n ℬ𝒾𝓃(6; p, n=9)\n\n\nIn REBayes, a similar analysis would use the Bmix function, but here we use our common fit interface with NPMLE. The information about the binomial likelihood is provided by the BinomialSample type instead, which makes the code more extensible and consistent across different likelihood models.\n\ntacks_npmle = fit(NPMLE(DiscretePriorClass(), Hypatia.Optimizer), tack_Zs);\n\n\n iter        p_obj        d_obj |  abs_gap    x_feas    z_feas |      tau       kap        mu | dir_res     prox  step     alpha\n    0   3.8484e-01  -5.0613e+00 | 3.19e+02  1.46e+00  1.01e+00 | 1.00e+00  1.00e+00  1.00e+00 |\n    1  -4.4676e+00  -6.8414e+00 | 9.56e+01  8.72e-01  6.01e-01 | 5.03e-01  7.39e-01  3.00e-01 | 1.8e-15  6.2e-01  co-a  7.00e-01\n    2  -2.0140e+00  -2.8978e+00 | 4.81e+01  2.01e-01  1.39e-01 | 1.09e+00  4.21e-03  1.50e-01 | 3.7e-15  9.7e-01  co-a  5.00e-01\n    3  -1.6879e+00  -2.0839e+00 | 3.35e+01  9.87e-02  6.81e-02 | 1.56e+00  6.11e-02  1.05e-01 | 7.1e-15  3.0e-01  co-a  3.00e-01\n    4  -1.4505e+00  -1.6458e+00 | 2.34e+01  4.72e-02  3.26e-02 | 2.28e+00  2.92e-02  7.34e-02 | 8.2e-15  3.3e-01  co-a  3.00e-01\n    5  -1.2610e+00  -1.3175e+00 | 1.17e+01  1.33e-02  9.16e-03 | 4.05e+00  8.18e-03  3.66e-02 | 1.1e-13  9.2e-01  co-a  5.00e-01\n    6  -1.1868e+00  -1.2061e+00 | 5.84e+00  4.50e-03  3.11e-03 | 5.97e+00  3.38e-03  1.83e-02 | 9.5e-13  9.2e-01  co-a  5.00e-01\n    7  -1.1831e+00  -1.2004e+00 | 5.25e+00  4.03e-03  2.78e-03 | 6.00e+00  2.75e-03  1.64e-02 | 1.1e-11  5.5e-01  co-a  1.00e-01\n    8  -1.1697e+00  -1.1810e+00 | 3.67e+00  2.63e-03  1.81e-03 | 6.45e+00  1.78e-03  1.15e-02 | 2.5e-12  5.1e-01  co-a  3.00e-01\n    9  -1.1609e+00  -1.1685e+00 | 2.56e+00  1.76e-03  1.21e-03 | 6.73e+00  1.19e-03  8.03e-03 | 4.6e-12  4.1e-01  co-a  3.00e-01\n   10  -1.1547e+00  -1.1599e+00 | 1.79e+00  1.20e-03  8.28e-04 | 6.91e+00  8.11e-04  5.61e-03 | 3.6e-12  4.0e-01  co-a  3.00e-01\n   11  -1.1503e+00  -1.1539e+00 | 1.25e+00  8.22e-04  5.67e-04 | 7.07e+00  5.54e-04  3.92e-03 | 1.2e-12  3.1e-01  co-a  3.00e-01\n   12  -1.1451e+00  -1.1468e+00 | 6.24e-01  3.98e-04  2.75e-04 | 7.29e+00  2.66e-04  1.95e-03 | 1.7e-12  7.0e-01  co-a  5.00e-01\n   13  -1.1436e+00  -1.1448e+00 | 4.35e-01  2.74e-04  1.89e-04 | 7.42e+00  1.83e-04  1.36e-03 | 1.5e-11  8.2e-01  co-a  3.00e-01\n   14  -1.1426e+00  -1.1434e+00 | 3.03e-01  1.88e-04  1.30e-04 | 7.57e+00  1.26e-04  9.50e-04 | 8.8e-12  4.9e-01  co-a  3.00e-01\n   15  -1.1419e+00  -1.1425e+00 | 2.12e-01  1.29e-04  8.91e-05 | 7.71e+00  8.62e-05  6.63e-04 | 1.9e-11  4.8e-01  co-a  3.00e-01\n   16  -1.1415e+00  -1.1418e+00 | 1.48e-01  8.91e-05  6.15e-05 | 7.82e+00  5.93e-05  4.63e-04 | 2.1e-11  4.9e-01  co-a  3.00e-01\n   17  -1.1412e+00  -1.1414e+00 | 1.03e-01  6.15e-05  4.24e-05 | 7.93e+00  4.09e-05  3.23e-04 | 9.8e-11  5.8e-01  co-a  3.00e-01\n   18  -1.1410e+00  -1.1411e+00 | 7.20e-02  4.26e-05  2.94e-05 | 8.01e+00  2.83e-05  2.26e-04 | 1.3e-10  6.3e-01  co-a  3.00e-01\n   19  -1.1408e+00  -1.1410e+00 | 5.02e-02  2.96e-05  2.04e-05 | 8.07e+00  1.96e-05  1.58e-04 | 1.1e-10  7.1e-01  co-a  3.00e-01\n   20  -1.1407e+00  -1.1408e+00 | 3.50e-02  2.07e-05  1.43e-05 | 8.10e+00  1.36e-05  1.10e-04 | 1.1e-10  7.0e-01  co-a  3.00e-01\n   21  -1.1407e+00  -1.1408e+00 | 2.45e-02  1.44e-05  9.96e-06 | 8.12e+00  9.48e-06  7.67e-05 | 9.5e-11  6.5e-01  co-a  3.00e-01\n   22  -1.1406e+00  -1.1407e+00 | 1.22e-02  7.22e-06  4.98e-06 | 8.12e+00  4.73e-06  3.81e-05 | 1.8e-10  8.3e-01  co-a  5.00e-01\n   23  -1.1406e+00  -1.1407e+00 | 8.46e-03  5.08e-06  3.51e-06 | 8.07e+00  3.31e-06  2.65e-05 | 8.7e-10  7.9e-01  co-a  3.00e-01\n   24  -1.1406e+00  -1.1406e+00 | 3.35e-03  2.05e-06  1.41e-06 | 8.00e+00  1.33e-06  1.05e-05 | 5.1e-10  6.7e-01  co-a  6.00e-01\n   25  -1.1406e+00  -1.1406e+00 | 1.31e-03  8.41e-07  5.80e-07 | 7.80e+00  5.45e-07  4.10e-06 | 3.7e-09  8.5e-01  co-a  6.00e-01\n   26  -1.1406e+00  -1.1406e+00 | 6.42e-04  4.29e-07  2.96e-07 | 7.65e+00  2.69e-07  2.01e-06 | 2.5e-09  8.4e-01  co-a  5.00e-01\n   27  -1.1406e+00  -1.1406e+00 | 3.18e-04  2.16e-07  1.49e-07 | 7.58e+00  1.33e-07  9.97e-07 | 1.1e-09  7.2e-01  co-a  5.00e-01\n   28  -1.1406e+00  -1.1406e+00 | 1.27e-04  8.69e-08  6.00e-08 | 7.55e+00  5.29e-08  3.97e-07 | 7.9e-10  8.6e-01  co-a  6.00e-01\n   29  -1.1406e+00  -1.1406e+00 | 2.53e-05  1.74e-08  1.20e-08 | 7.54e+00  1.05e-08  7.93e-08 | 3.1e-10  8.2e-01  co-a  8.00e-01\n   30  -1.1406e+00  -1.1406e+00 | 1.01e-05  7.00e-09  4.82e-09 | 7.51e+00  4.23e-09  3.16e-08 | 8.2e-10  2.6e-01  co-a  6.00e-01\n   31  -1.1406e+00  -1.1406e+00 | 9.08e-06  6.29e-09  4.34e-09 | 7.51e+00  3.79e-09  2.85e-08 | 3.5e-09  9.3e-02  co-a  1.00e-01\n   32  -1.1406e+00  -1.1406e+00 | 8.62e-06  5.99e-09  4.12e-09 | 7.51e+00  3.60e-09  2.70e-08 | 1.9e-08  2.4e-02  co-a  5.00e-02\n   33  -1.1406e+00  -1.1406e+00 | 8.53e-06  5.93e-09  4.08e-09 | 7.51e+00  3.56e-09  2.68e-08 | 5.5e-10  1.0e-05  co-a  1.00e-02\n   34  -1.1406e+00  -1.1406e+00 | 5.97e-06  4.15e-09  2.86e-09 | 7.51e+00  2.49e-09  1.87e-08 | 3.3e-10  6.1e-04  co-a  3.00e-01\n   35  -1.1406e+00  -1.1406e+00 | 5.91e-06  4.11e-09  2.83e-09 | 7.51e+00  2.47e-09  1.85e-08 | 3.0e-10  2.2e-05  co-a  1.00e-02\n   36  -1.1406e+00  -1.1406e+00 | 4.14e-06  2.86e-09  1.98e-09 | 7.51e+00  1.73e-09  1.30e-08 | 3.0e-09  9.2e-03  co-a  3.00e-01\noptimal solution found; terminating\n\nstatus is Optimal after 36 iterations and 0.18 seconds\n\n\n\nLet’s visualize the estimated prior distribution of success probabilities:\n\nplot(support(tacks_npmle.prior),     \n     probs(tacks_npmle.prior),   \n     seriestype=:sticks,     \n     xguide = \"p\",       \n     yguide = \"g(p)\",    \n     size=(500,400),     \n     label=nothing,\n     title=\"NPMLE prior for Tacks data\")\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.2: Estimated prior distribution for the Tacks dataset showing the probability of a tack landing point-up.\n\n\n\n\nThe plot shows the estimated distribution of success probabilities for the tacks. The NPMLE identifies several mass points, indicating groups of tacks with different tendencies to land point-up.",
    "crumbs": [
      "Vignettes",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Empirikos.jl: Reproducing REBayes Examples</span>"
    ]
  },
  {
    "objectID": "vignettes/REBayes.html#poisson-npmle",
    "href": "vignettes/REBayes.html#poisson-npmle",
    "title": "Empirikos.jl: Reproducing REBayes Examples",
    "section": "Poisson NPMLE",
    "text": "Poisson NPMLE\nFinally, we’ll analyze the Norberg insurance claims dataset, which consists of claim counts and exposure times for different occupational groups.\n\nnorberg_tbl = Empirikos.Norberg.load_table()\n\n72-element CSV.File:\n CSV.Row: (OccGroup = 1, Exposure = 3349.02, Death = 16)\n CSV.Row: (OccGroup = 2, Exposure = 1394.0, Death = 5)\n CSV.Row: (OccGroup = 3, Exposure = 479.81, Death = 0)\n CSV.Row: (OccGroup = 4, Exposure = 23.98, Death = 0)\n CSV.Row: (OccGroup = 5, Exposure = 11.3, Death = 0)\n CSV.Row: (OccGroup = 6, Exposure = 2273.55, Death = 11)\n CSV.Row: (OccGroup = 7, Exposure = 179.85, Death = 0)\n CSV.Row: (OccGroup = 8, Exposure = 3947.44, Death = 24)\n CSV.Row: (OccGroup = 9, Exposure = 4332.56, Death = 9)\n CSV.Row: (OccGroup = 10, Exposure = 109.84, Death = 1)\n CSV.Row: (OccGroup = 11, Exposure = 647.77, Death = 2)\n CSV.Row: (OccGroup = 12, Exposure = 1308.36, Death = 1)\n CSV.Row: (OccGroup = 13, Exposure = 154.81, Death = 4)\n ⋮\n CSV.Row: (OccGroup = 61, Exposure = 1551.3, Death = 3)\n CSV.Row: (OccGroup = 62, Exposure = 113.49, Death = 0)\n CSV.Row: (OccGroup = 63, Exposure = 147.44, Death = 0)\n CSV.Row: (OccGroup = 64, Exposure = 826.4, Death = 2)\n CSV.Row: (OccGroup = 65, Exposure = 775.19, Death = 1)\n CSV.Row: (OccGroup = 66, Exposure = 634.16, Death = 2)\n CSV.Row: (OccGroup = 67, Exposure = 2293.45, Death = 6)\n CSV.Row: (OccGroup = 68, Exposure = 942.03, Death = 4)\n CSV.Row: (OccGroup = 69, Exposure = 232.31, Death = 2)\n CSV.Row: (OccGroup = 70, Exposure = 37.08, Death = 0)\n CSV.Row: (OccGroup = 71, Exposure = 233.25, Death = 0)\n CSV.Row: (OccGroup = 72, Exposure = 39.52, Death = 0)\n\n\nWe convert the data to PoissonSample objects. Each sample captures both the count (death) and the exposure time:\n\nnorberg_Zs = Empirikos.PoissonSample.(norberg_tbl.Death, norberg_tbl.Exposure ./ 344);\n\nLet’s examine the range of claim rates:\n\nextrema(response.(norberg_Zs) ./ nuisance_parameter.(norberg_Zs))\n\n(0.0, 8.888314708352173)\n\n\nAnd check how many groups had zero claims:\n\nsum(response.(norberg_Zs) .== 0) # how many zeros in the samples?\n\n23\n\n\nNow we fit the NPMLE with a finer grid of 1000 points:\n\nnorberg_npmle = fit(NPMLE(DiscretePriorClass(),                           \n                    Hypatia.Optimizer;                        \n                    prior_grid_length=1000),                    \n                    norberg_Zs);\n\n\n iter        p_obj        d_obj |  abs_gap    x_feas    z_feas |      tau       kap        mu | dir_res     prox  step     alpha\n    0   1.2878e-01  -2.3832e+01 | 4.45e+02  1.29e+01  6.83e-01 | 1.00e+00  1.00e+00  1.00e+00 |\n    1  -4.8042e+00  -2.8956e+01 | 3.11e+02  1.32e+01  7.03e-01 | 6.80e-01  1.05e+00  7.00e-01 | 1.4e-14  2.9e-01  co-a  3.00e-01\n    2  -3.7142e+01  -5.6908e+01 | 1.25e+02  1.19e+01  6.30e-01 | 3.04e-01  9.84e-01  2.80e-01 | 1.4e-14  9.7e-01  co-a  6.00e-01\n    3  -3.9425e+01  -5.3262e+01 | 8.74e+01  8.14e+00  4.33e-01 | 3.10e-01  6.10e-01  1.96e-01 | 1.5e-14  6.6e-01  co-a  3.00e-01\n    4  -2.3077e+01  -2.8299e+01 | 4.36e+01  2.95e+00  1.57e-01 | 4.27e-01  2.15e-01  9.80e-02 | 5.0e-14  6.9e-01  co-a  5.00e-01\n    5  -1.7909e+01  -2.1253e+01 | 3.05e+01  1.88e+00  1.00e-01 | 4.69e-01  1.44e-01  6.84e-02 | 7.9e-14  6.1e-01  co-a  3.00e-01\n    6  -1.4033e+01  -1.6182e+01 | 2.13e+01  1.20e+00  6.36e-02 | 5.16e-01  9.08e-02  4.78e-02 | 3.3e-14  4.8e-01  co-a  3.00e-01\n    7  -1.1359e+01  -1.2814e+01 | 1.48e+01  8.09e-01  4.30e-02 | 5.34e-01  6.18e-02  3.33e-02 | 5.7e-14  6.0e-01  co-a  3.00e-01\n    8  -9.1893e+00  -1.0156e+01 | 1.03e+01  5.35e-01  2.84e-02 | 5.66e-01  4.04e-02  2.33e-02 | 4.3e-14  6.6e-01  co-a  3.00e-01\n    9  -7.5456e+00  -8.1914e+00 | 7.21e+00  3.56e-01  1.89e-02 | 5.95e-01  2.67e-02  1.62e-02 | 8.7e-14  9.6e-01  co-a  3.00e-01\n   10  -6.9671e+00  -7.5117e+00 | 6.48e+00  2.99e-01  1.59e-02 | 6.38e-01  2.26e-02  1.46e-02 | 1.9e-13  5.1e-01  co-a  1.00e-01\n   11  -5.7868e+00  -6.1320e+00 | 4.52e+00  1.88e-01  9.99e-03 | 7.10e-01  1.39e-02  1.02e-02 | 6.8e-14  6.1e-01  co-a  3.00e-01\n   12  -4.9867e+00  -5.2060e+00 | 3.15e+00  1.19e-01  6.31e-03 | 7.87e-01  8.82e-03  7.07e-03 | 1.1e-13  7.9e-01  co-a  3.00e-01\n   13  -4.4583e+00  -4.5972e+00 | 2.19e+00  7.48e-02  3.98e-03 | 8.74e-01  5.55e-03  4.93e-03 | 2.2e-13  6.3e-01  co-a  3.00e-01\n   14  -4.1229e+00  -4.2119e+00 | 1.53e+00  4.77e-02  2.54e-03 | 9.59e-01  3.55e-03  3.43e-03 | 2.4e-13  5.1e-01  co-a  3.00e-01\n   15  -3.9121e+00  -3.9696e+00 | 1.07e+00  3.08e-02  1.64e-03 | 1.04e+00  2.29e-03  2.40e-03 | 4.8e-13  3.6e-01  co-a  3.00e-01\n   16  -3.7835e+00  -3.8213e+00 | 7.45e-01  2.02e-02  1.07e-03 | 1.11e+00  1.50e-03  1.67e-03 | 4.5e-13  3.5e-01  co-a  3.00e-01\n   17  -3.7039e+00  -3.7291e+00 | 5.20e-01  1.34e-02  7.12e-04 | 1.17e+00  9.99e-04  1.17e-03 | 5.0e-13  4.1e-01  co-a  3.00e-01\n   18  -3.6540e+00  -3.6708e+00 | 3.63e-01  8.98e-03  4.77e-04 | 1.22e+00  6.68e-04  8.17e-04 | 1.4e-12  5.0e-01  co-a  3.00e-01\n   19  -3.6222e+00  -3.6336e+00 | 2.54e-01  6.06e-03  3.22e-04 | 1.27e+00  4.50e-04  5.70e-04 | 2.4e-12  6.2e-01  co-a  3.00e-01\n   20  -3.6019e+00  -3.6096e+00 | 1.77e-01  4.11e-03  2.18e-04 | 1.31e+00  3.05e-04  3.98e-04 | 4.5e-12  7.9e-01  co-a  3.00e-01\n   21  -3.5980e+00  -3.6049e+00 | 1.59e-01  3.67e-03  1.95e-04 | 1.32e+00  2.72e-04  3.58e-04 | 8.6e-12  4.5e-01  co-a  1.00e-01\n   22  -3.5867e+00  -3.5915e+00 | 1.11e-01  2.51e-03  1.33e-04 | 1.35e+00  1.85e-04  2.50e-04 | 5.9e-12  5.7e-01  co-a  3.00e-01\n   23  -3.5793e+00  -3.5825e+00 | 7.76e-02  1.71e-03  9.07e-05 | 1.39e+00  1.26e-04  1.74e-04 | 6.9e-12  8.5e-01  co-a  3.00e-01\n   24  -3.5780e+00  -3.5809e+00 | 6.98e-02  1.53e-03  8.14e-05 | 1.40e+00  1.13e-04  1.57e-04 | 1.1e-11  3.7e-01  co-a  1.00e-01\n   25  -3.5740e+00  -3.5760e+00 | 4.88e-02  1.05e-03  5.58e-05 | 1.43e+00  7.70e-05  1.10e-04 | 4.4e-12  5.6e-01  co-a  3.00e-01\n   26  -3.5715e+00  -3.5728e+00 | 3.41e-02  7.20e-04  3.83e-05 | 1.45e+00  5.28e-05  7.65e-05 | 1.5e-11  8.4e-01  co-a  3.00e-01\n   27  -3.5710e+00  -3.5723e+00 | 3.06e-02  6.46e-04  3.43e-05 | 1.46e+00  4.72e-05  6.88e-05 | 2.4e-11  3.6e-01  co-a  1.00e-01\n   28  -3.5697e+00  -3.5705e+00 | 2.14e-02  4.45e-04  2.37e-05 | 1.48e+00  3.25e-05  4.81e-05 | 1.3e-11  6.0e-01  co-a  3.00e-01\n   29  -3.5687e+00  -3.5693e+00 | 1.50e-02  3.07e-04  1.63e-05 | 1.51e+00  2.24e-05  3.36e-05 | 6.2e-11  9.5e-01  co-a  3.00e-01\n   30  -3.5686e+00  -3.5691e+00 | 1.34e-02  2.76e-04  1.46e-05 | 1.51e+00  2.01e-05  3.02e-05 | 3.9e-11  4.2e-01  co-a  1.00e-01\n   31  -3.5681e+00  -3.5684e+00 | 9.39e-03  1.91e-04  1.01e-05 | 1.53e+00  1.39e-05  2.11e-05 | 3.7e-11  7.3e-01  co-a  3.00e-01\n   32  -3.5680e+00  -3.5683e+00 | 8.44e-03  1.71e-04  9.09e-06 | 1.53e+00  1.24e-05  1.90e-05 | 1.0e-10  3.8e-01  co-a  1.00e-01\n   33  -3.5677e+00  -3.5679e+00 | 5.90e-03  1.19e-04  6.31e-06 | 1.54e+00  8.60e-06  1.33e-05 | 2.5e-11  7.0e-01  co-a  3.00e-01\n   34  -3.5676e+00  -3.5678e+00 | 5.31e-03  1.07e-04  5.66e-06 | 1.55e+00  7.70e-06  1.19e-05 | 9.6e-11  3.9e-01  co-a  1.00e-01\n   35  -3.5674e+00  -3.5675e+00 | 3.71e-03  7.40e-05  3.93e-06 | 1.56e+00  5.35e-06  8.33e-06 | 3.1e-11  7.3e-01  co-a  3.00e-01\n   36  -3.5674e+00  -3.5675e+00 | 3.34e-03  6.65e-05  3.53e-06 | 1.56e+00  4.80e-06  7.50e-06 | 1.3e-10  4.1e-01  co-a  1.00e-01\n   37  -3.5673e+00  -3.5673e+00 | 2.33e-03  4.63e-05  2.46e-06 | 1.57e+00  3.34e-06  5.24e-06 | 7.0e-11  7.5e-01  co-a  3.00e-01\n   38  -3.5672e+00  -3.5673e+00 | 2.10e-03  4.16e-05  2.21e-06 | 1.57e+00  2.99e-06  4.71e-06 | 1.9e-10  4.2e-01  co-a  1.00e-01\n   39  -3.5672e+00  -3.5672e+00 | 1.46e-03  2.90e-05  1.54e-06 | 1.58e+00  2.09e-06  3.29e-06 | 6.9e-11  7.4e-01  co-a  3.00e-01\n   40  -3.5671e+00  -3.5672e+00 | 1.32e-03  2.61e-05  1.39e-06 | 1.58e+00  1.87e-06  2.96e-06 | 3.9e-10  4.0e-01  co-a  1.00e-01\n   41  -3.5671e+00  -3.5671e+00 | 9.20e-04  1.82e-05  9.67e-07 | 1.59e+00  1.31e-06  2.07e-06 | 1.2e-10  6.6e-01  co-a  3.00e-01\n   42  -3.5671e+00  -3.5671e+00 | 6.43e-04  1.27e-05  6.76e-07 | 1.59e+00  9.11e-07  1.45e-06 | 3.9e-10  8.5e-01  co-a  3.00e-01\n   43  -3.5671e+00  -3.5671e+00 | 5.78e-04  1.15e-05  6.08e-07 | 1.59e+00  8.19e-07  1.30e-06 | 2.2e-10  3.6e-01  co-a  1.00e-01\n   44  -3.5670e+00  -3.5671e+00 | 4.04e-04  8.00e-06  4.25e-07 | 1.59e+00  5.72e-07  9.09e-07 | 1.5e-10  4.4e-01  co-a  3.00e-01\n   45  -3.5670e+00  -3.5670e+00 | 2.83e-04  5.60e-06  2.97e-07 | 1.59e+00  3.99e-07  6.36e-07 | 1.1e-10  4.7e-01  co-a  3.00e-01\n   46  -3.5670e+00  -3.5670e+00 | 1.41e-04  2.80e-06  1.49e-07 | 1.59e+00  2.00e-07  3.17e-07 | 1.4e-10  9.9e-01  co-a  5.00e-01\n   47  -3.5670e+00  -3.5670e+00 | 9.86e-05  1.96e-06  1.04e-07 | 1.59e+00  1.40e-07  2.22e-07 | 7.9e-10  7.4e-01  co-a  3.00e-01\n   48  -3.5670e+00  -3.5670e+00 | 4.92e-05  9.82e-07  5.22e-08 | 1.59e+00  6.98e-08  1.10e-07 | 3.7e-10  8.8e-01  co-a  5.00e-01\n   49  -3.5670e+00  -3.5670e+00 | 1.47e-05  2.96e-07  1.57e-08 | 1.58e+00  2.10e-08  3.30e-08 | 3.0e-10  9.0e-01  co-a  7.00e-01\n   50  -3.5670e+00  -3.5670e+00 | 7.28e-06  1.49e-07  7.93e-09 | 1.57e+00  1.05e-08  1.64e-08 | 2.2e-09  9.4e-01  co-a  5.00e-01\n   51  -3.5670e+00  -3.5670e+00 | 3.63e-06  7.49e-08  3.98e-09 | 1.56e+00  5.24e-09  8.15e-09 | 1.3e-09  7.6e-01  co-a  5.00e-01\n   52  -3.5670e+00  -3.5670e+00 | 1.14e-06  2.52e-08  1.23e-09 | 1.52e+00  1.69e-09  2.57e-09 | 3.7e-08  5.9e-01  co-a  7.00e-01\n   53  -3.5670e+00  -3.5670e+00 | 3.45e-07  7.60e-09  3.68e-10 | 1.52e+00  5.09e-10  7.75e-10 | 3.7e-09  1.1e-01  co-a  7.00e-01\noptimal solution found; terminating\n\nstatus is Optimal after 53 iterations and 0.281 seconds\n\n\n\nIn REBayes, this would be accomplished with the Pmix function with exposure arguments, but our approach provides a unified interface with the type system handling the exposure information.\nLet’s visualize the results:\n\npl = plot(\n        plot(support(norberg_npmle.prior),        \n            probs(norberg_npmle.prior)./sum(probs(norberg_npmle.prior)),          \n            seriestype=:sticks,           \n            xguide = L\"\\theta\",           \n            yguide = L\"g(\\theta)\",            \n            legend=:topright,         \n            label=\"NPMLE\",\n            title=\"Risk distribution\"), \n        plot(support(norberg_npmle.prior),        \n            (probs(norberg_npmle.prior) ./ sum(probs(norberg_npmle.prior))).^(1/3),           \n            seriestype=:sticks,           \n            xguide = L\"\\theta\",           \n            yguide = L\"g(\\theta)^{1/3}\",          \n            legend=:topright,         \n            label=\"NPMLE\",\n            title=\"Cube-root transformed\"),     \n            size=(800,400)\n        )\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.3: Estimated risk distribution for insurance claims data. Left: Original scale. Right: Cube-root transformed to better visualize smaller masses.\n\n\n\n\nThe left panel shows the estimated risk distribution, while the right panel shows the cube-root transformation to better visualize smaller masses. Similar to the REBayes analysis, we can see evidence of groups with particularly high risk (around θ=8), which would be difficult to capture with a parametric approach.\nFinally, we compute the posterior means for each group:\n\nnorgberg_postmean_npmle = (PosteriorMean.(norberg_Zs)).(norberg_npmle.prior)\n\n72-element Vector{Float64}:\n 1.516497463019897\n 1.1477304056884876\n 0.8900678400349428\n 1.1229084038155277\n 1.1343002295969666\n 1.4664836253077358\n 1.0164762017419169\n 1.6494798867585212\n 0.7913509956260509\n 1.2713016956237706\n 1.0779960615909798\n 0.7848319618996998\n 2.4249473789626075\n ⋮\n 0.8706989730386656\n 1.0562150646243584\n 1.035113175842552\n 0.9955905035611794\n 0.8996424331413105\n 1.085110916237371\n 0.9348278530015118\n 1.2292844609156781\n 1.3854937394831124\n 1.1117890803224781\n 0.9884559917026545\n 1.1097830775100936\n\n\nThese values represent the empirical Bayes credibility estimates for each occupation group’s risk factor. In insurance terminology, these would be used to set premiums that reflect both the overall portfolio patterns and each group’s specific experience.\n\n# Display the mean posterior estimate \nmean(norgberg_postmean_npmle)\n\n1.14506213208152\n\n\nThis mean value represents the overall risk level across all occupational groups after empirical Bayes adjustment.",
    "crumbs": [
      "Vignettes",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Empirikos.jl: Reproducing REBayes Examples</span>"
    ]
  },
  {
    "objectID": "vignettes/prostate.html",
    "href": "vignettes/prostate.html",
    "title": "Identifying genes associated with prostate cancer",
    "section": "",
    "text": "Setup and data loading\nusing DataFrames\nusing Distributions\nusing Empirikos\nusing Hypatia\nusing Plots\nusing LaTeXStrings\nusing IntervalSets\nusing JuMP\nusing Random\n\n# Set up pgfplotsx and custom preamble\ngr()\n\n# Set plot theme\ntheme(:default;\n      background_color_legend = :transparent,\n      foreground_color_legend = :transparent,\n      grid=nothing,\n      frame=:box,\n      legendfonthalign = :left,\n      thickness_scaling=1.3)\n# Load the prostate data\nZs = Prostate.ebayes_samples()\n\n6033-element Vector{StandardNormalSample{Float64}}:\n N(1.47236666651029; μ, σ=1.0)\n N(3.57291516191986; μ, σ=1.0)\n N(-0.0277536813375331; μ, σ=1.0)\n N(-1.1320516549926; μ, σ=1.0)\n N(-0.140220986707002; μ, σ=1.0)\n N(0.958809840576886; μ, σ=1.0)\n N(1.0681232949828; μ, σ=1.0)\n N(-1.2914820655344; μ, σ=1.0)\n N(-1.22750497688098; μ, σ=1.0)\n N(1.17605516919472; μ, σ=1.0)\n N(3.34583078878689; μ, σ=1.0)\n N(0.188740669060314; μ, σ=1.0)\n N(-0.851333492603207; μ, σ=1.0)\n ⋮\n N(0.55060618889427; μ, σ=1.0)\n N(0.265461705731843; μ, σ=1.0)\n N(-1.19340060387477; μ, σ=1.0)\n N(-1.83473655530782; μ, σ=1.0)\n N(0.0707678142839226; μ, σ=1.0)\n N(0.0243971545443552; μ, σ=1.0)\n N(-0.207896831967974; μ, σ=1.0)\n N(0.287356609464891; μ, σ=1.0)\n N(-0.777423419217064; μ, σ=1.0)\n N(-1.18366655761165; μ, σ=1.0)\n N(0.103544128661099; μ, σ=1.0)\n N(-0.909192514625776; μ, σ=1.0)",
    "crumbs": [
      "Vignettes",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Identifying genes associated with prostate cancer</span>"
    ]
  },
  {
    "objectID": "vignettes/prostate.html#marginal-distribution-of-z-scores",
    "href": "vignettes/prostate.html#marginal-distribution-of-z-scores",
    "title": "Identifying genes associated with prostate cancer",
    "section": "Marginal distribution of z-scores",
    "text": "Marginal distribution of z-scores\n\nDKW-F-Localization\n\n# Create and fit the DKW F-Localization\ndkw_floc = DvoretzkyKieferWolfowitz(0.05)\nfitted_dkw = fit(dkw_floc, Zs)\n\n# Plot the DKW band\ndkw_plot = plot(fitted_dkw, label=\"DKW band\",\n     xlab=L\"z\", ylab=L\"\\widehat{F}_n(z)\",  size=(380,280))\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKDE-F-Localization\n\n# Create and fit the KDE-based F-Localization\ninfty_floc = Empirikos.InfinityNormDensityBand(α=0.05)\nfitted_infty_floc = fit(infty_floc, Zs)\n\n# Plot the KDE band with histogram\nprostate_marginal_plot = histogram([response(Z) for Z in Zs],\n    bins=50, normalize=true,\n    label=\"Histogram\", fillalpha=0.4, linealpha=0.4, fillcolor=:lightgray,\n    size=(380,280), xlims=(-4.5,4.5))\n\nplot!(prostate_marginal_plot, fitted_infty_floc,\n      label=\"KDE band\", xlims=(-4.5,4.5),\n      yguide=L\"\\widehat{f}_n(z)\", xguide=L\"z\")\n\nplot!(prostate_marginal_plot, [fitted_infty_floc.a_min; fitted_infty_floc.a_max], seriestype=:vline,\n      linestyle=:dot, label=nothing, color=:lightgrey)",
    "crumbs": [
      "Vignettes",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Identifying genes associated with prostate cancer</span>"
    ]
  },
  {
    "objectID": "vignettes/prostate.html#confidence-intervals",
    "href": "vignettes/prostate.html#confidence-intervals",
    "title": "Identifying genes associated with prostate cancer",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\n# Define prior classes\ngcal_scalemix = Empirikos.autoconvexclass(GaussianScaleMixtureClass(), Zs; :grid_scaling =&gt; 1.1)\n\nGaussianScaleMixtureClass | σs = [0.1, 0.11, 0.121, 0.1331, 0.14641, 0.161051, 0.177156, 0.194872, 0.214359, 0.235795  …  4.52593, 4.97852, 5.47637, 6.02401, 6.62641, 7.28905, 8.01795, 8.81975, 9.70172, 10.6719]\n\n\n\nF-Localization methods setup\n\nfloc_method_dkw_scalemix = FLocalizationInterval(\n    flocalization = dkw_floc,\n    convexclass = gcal_scalemix, \n    solver = Hypatia.Optimizer\n)\n\nfloc_method_kde_scalemix = FLocalizationInterval(\n    flocalization = infty_floc,\n    convexclass = gcal_scalemix, \n    solver = Hypatia.Optimizer\n)\n\nEB intervals with F-Localization: ∞-density band [α: 0.05] [Kernel: FlatTopKernel | bandwidth = nothing] [Bootstrap: Multinomial(1000)]\n                  𝒢: GaussianScaleMixtureClass | σs = [0.1, 0.11, 0.121, 0.1331, 0.14641, 0.161051, 0.177156, 0.194872, 0.214359, 0.235795  …  4.52593, 4.97852, 5.47637, 6.02401, 6.62641, 7.28905, 8.01795, 8.81975, 9.70172, 10.6719]\n\n\n\n\nCompute confidence intervals\n\n# Define values for which to compute confidence intervals\nts = -3:0.25:3\n\n# Define posterior targets\npostmean_targets = Empirikos.PosteriorMean.(StandardNormalSample.(ts))\n\n25-element Vector{PosteriorMean{StandardNormalSample{Float64}}}:\n 𝔼[μ | N(-3.0; μ, σ=1.0)]\n 𝔼[μ | N(-2.75; μ, σ=1.0)]\n 𝔼[μ | N(-2.5; μ, σ=1.0)]\n 𝔼[μ | N(-2.25; μ, σ=1.0)]\n 𝔼[μ | N(-2.0; μ, σ=1.0)]\n 𝔼[μ | N(-1.75; μ, σ=1.0)]\n 𝔼[μ | N(-1.5; μ, σ=1.0)]\n 𝔼[μ | N(-1.25; μ, σ=1.0)]\n 𝔼[μ | N(-1.0; μ, σ=1.0)]\n 𝔼[μ | N(-0.75; μ, σ=1.0)]\n 𝔼[μ | N(-0.5; μ, σ=1.0)]\n 𝔼[μ | N(-0.25; μ, σ=1.0)]\n 𝔼[μ | N(0.0; μ, σ=1.0)]\n 𝔼[μ | N(0.25; μ, σ=1.0)]\n 𝔼[μ | N(0.5; μ, σ=1.0)]\n 𝔼[μ | N(0.75; μ, σ=1.0)]\n 𝔼[μ | N(1.0; μ, σ=1.0)]\n 𝔼[μ | N(1.25; μ, σ=1.0)]\n 𝔼[μ | N(1.5; μ, σ=1.0)]\n 𝔼[μ | N(1.75; μ, σ=1.0)]\n 𝔼[μ | N(2.0; μ, σ=1.0)]\n 𝔼[μ | N(2.25; μ, σ=1.0)]\n 𝔼[μ | N(2.5; μ, σ=1.0)]\n 𝔼[μ | N(2.75; μ, σ=1.0)]\n 𝔼[μ | N(3.0; μ, σ=1.0)]\n\n\nCompute confidence intervals for scale mixture priors\n\npostmean_ci_dkw_scalemix = confint.(floc_method_dkw_scalemix, postmean_targets, Zs)\n\n\npostmean_ci_kde_scalemix = confint.(floc_method_kde_scalemix, postmean_targets, Zs)\n\n\n\nVisualize confidence intervals for scale mixture priors\n\npostmean_scalemix_plot = plot(ts, postmean_ci_kde_scalemix,\n    label=\"Gauss-F-Loc\", fillcolor=:darkorange, fillalpha=0.5, ylim=(-2.55,2.55),\n    xguide = L\"z\", yguide=L\"E[\\mu \\mid Z=z]\",\n    size=(380,280))\n\nplot!(postmean_scalemix_plot, ts, postmean_ci_dkw_scalemix,\n    label=\"DKW-F-Loc\", show_ribbon=false, alpha=0.9, color=:black)\n\nplot!(postmean_scalemix_plot, [-3.0;3.0], [-3.0; 3.0], seriestype=:line,\n    linestyle=:dot, label=nothing, color=:lightgrey)",
    "crumbs": [
      "Vignettes",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Identifying genes associated with prostate cancer</span>"
    ]
  }
]